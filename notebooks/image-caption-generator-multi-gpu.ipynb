{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111749,"sourceType":"datasetVersion","datasetId":623329}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting Up","metadata":{}},{"cell_type":"markdown","source":"## Installing requirements","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T11:51:22.890154Z","iopub.execute_input":"2024-12-01T11:51:22.890505Z","iopub.status.idle":"2024-12-01T11:51:23.941063Z","shell.execute_reply.started":"2024-12-01T11:51:22.890472Z","shell.execute_reply":"2024-12-01T11:51:23.939845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tensorflow keras tensorboard scikit-learn ipywidgets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport os\nimport sys\n\nimport random\nfrom PIL import Image \n\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport keras\nfrom keras.applications.vgg19 import VGG19, preprocess_input\nfrom keras.layers import Input, Dense, Dropout, Embedding, LSTM, add, Flatten\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.utils import to_categorical, plot_model\nfrom keras import Model\n\nfrom sklearn.model_selection import train_test_split\n\n# notebook specific\nfrom IPython.display import display\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:08:59.901606Z","iopub.execute_input":"2024-12-02T13:08:59.901966Z","iopub.status.idle":"2024-12-02T13:09:11.395276Z","shell.execute_reply.started":"2024-12-02T13:08:59.901926Z","shell.execute_reply":"2024-12-02T13:09:11.394604Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Some Parameters","metadata":{}},{"cell_type":"code","source":"# constants\nTRAIN_CNN: bool = False\nNUM_OUTPUT_CAPTIONS: int = 1\nIMAGE_INPUT_SHAPE: tuple[int, int, int] = (224, 224, 3) # (height, width, channel)\nDATASET_NAME: str = \"flickr30k\"\n\nFILTER_NON_ALPHA_NUMERIC_STRINGS = True\n\nEVALUATE_AFTER_TRAIN: bool = False\nLOAD_PRETRAINED: bool = False # make it possible to continue training from a saved model\nUSE_MULTIPLE_GPUS: bool = True \n\n# Hyperparameters\nN_EPOCHS: int = 5\nBATCH_SIZE: int = 64\nDEBUG: bool = False\nTRAIN_TEST_VAL_SPLIT = (70, 20, 10)\n    \n# paths\nWORKING_DIR = Path(\"/kaggle/working\")\nDATASET_INPUT_DIR = Path(\"/kaggle/input/\").joinpath(DATASET_NAME)\nTEMP_DIR = Path(\"/tmp\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:11.396557Z","iopub.execute_input":"2024-12-02T13:09:11.396980Z","iopub.status.idle":"2024-12-02T13:09:11.402377Z","shell.execute_reply.started":"2024-12-02T13:09:11.396952Z","shell.execute_reply":"2024-12-02T13:09:11.401379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\n\n# setting up the logger\nlogging.basicConfig(level=logging.DEBUG if DEBUG else logging.INFO, force=True) # a workaround\nlogger = logging.getLogger(\"-^-\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:11.403598Z","iopub.execute_input":"2024-12-02T13:09:11.403949Z","iopub.status.idle":"2024-12-02T13:09:11.480428Z","shell.execute_reply.started":"2024-12-02T13:09:11.403906Z","shell.execute_reply":"2024-12-02T13:09:11.479611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_good = True\n\nif not os.path.exists(WORKING_DIR):\n    logger.error(f\"The directory {WORKING_DIR} doesn't exist.\")\n    all_good = False\n\nif not os.path.exists(DATASET_INPUT_DIR):\n    logger.error(f\"The directory {DATASET_INPUT_DIR} doesn't exist.\")\n    all_good = False\n\nif not os.path.exists(TEMP_DIR):\n    logger.error(f\"The directory {TEMP_DIR} doesn't exist.\")\n    all_good = False\n\nif all_good:\n    logger.info(f\"All the directories are valid.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:11.481981Z","iopub.execute_input":"2024-12-02T13:09:11.482221Z","iopub.status.idle":"2024-12-02T13:09:11.504314Z","shell.execute_reply.started":"2024-12-02T13:09:11.482197Z","shell.execute_reply":"2024-12-02T13:09:11.503584Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Preparation","metadata":{}},{"cell_type":"code","source":"images_dir = DATASET_INPUT_DIR.joinpath(\"Images\")\ncaptions_file = DATASET_INPUT_DIR.joinpath(\"captions.txt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:11.505226Z","iopub.execute_input":"2024-12-02T13:09:11.505465Z","iopub.status.idle":"2024-12-02T13:09:11.509036Z","shell.execute_reply.started":"2024-12-02T13:09:11.505441Z","shell.execute_reply":"2024-12-02T13:09:11.508275Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating the vocabulary","metadata":{}},{"cell_type":"code","source":"captions_data = pd.read_csv(captions_file)\ncaptions_data.astype(str)\ncaptions_data.dropna(inplace=True)\nlogger.info(captions_data.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:11.510052Z","iopub.execute_input":"2024-12-02T13:09:11.510483Z","iopub.status.idle":"2024-12-02T13:09:11.900429Z","shell.execute_reply.started":"2024-12-02T13:09:11.510443Z","shell.execute_reply":"2024-12-02T13:09:11.899551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# filtering the columns having too few or too many words\n# as having too few and too many can improperly skew whole training process\n# having too many causes the whole network to be trained mostly on padding rather than the actual data\n\nupper_limit = 30\nlower_limit = 6\n\ncaptions_data = captions_data.drop(captions_data.loc[captions_data[\"caption\"].apply(lambda x: len(str(x).split()) > upper_limit or len(str(x).split()) < lower_limit)].index)\n\nlen(captions_data[\"image\"].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:11.901428Z","iopub.execute_input":"2024-12-02T13:09:11.901679Z","iopub.status.idle":"2024-12-02T13:09:12.149194Z","shell.execute_reply.started":"2024-12-02T13:09:11.901652Z","shell.execute_reply":"2024-12-02T13:09:12.148399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unfiltered_vocabulary = list(set(\" \".join(captions_data[\"caption\"].to_list()).lower().split()))\n\nremoved_items = []\n\nif FILTER_NON_ALPHA_NUMERIC_STRINGS:\n    vocabulary = list(filter(lambda x: len(x) >= 3 or x.isalpha() , unfiltered_vocabulary))\n    removed_items += list(filter(lambda x: len(x) < 3 and not x.isalpha() , unfiltered_vocabulary))\nelse:\n    vocabulary = unfiltered_vocabulary\n\nfiltered_captions_data = captions_data.copy()\nfiltered_captions_data[\"caption\"] = filtered_captions_data[\"caption\"].apply(\n    lambda x: \" \".join(list(filter(\n        lambda y: y not in removed_items, x.lower().split()))\n                      )\n)\n\nvocabulary_from_filtered_captions_data = list(set(\" \".join(filtered_captions_data[\"caption\"].to_list()).lower().split()))\n\nfor x in vocabulary_from_filtered_captions_data:\n    if x not in vocabulary:\n        logger.error(f\"Word: '{x}' not in vocabulary\")\n        raise Exception(\"Found a word that is not in the vocabulary\")\n\ncaptions_data = filtered_captions_data\n\nlogger.info(f\"Removed items: {removed_items}\")\n\nlogger.info(f\"Total unique words: {len(vocabulary)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:12.150338Z","iopub.execute_input":"2024-12-02T13:09:12.150708Z","iopub.status.idle":"2024-12-02T13:09:20.317727Z","shell.execute_reply.started":"2024-12-02T13:09:12.150669Z","shell.execute_reply":"2024-12-02T13:09:20.316873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VocabHandler():\n    \"\"\"\n    Handles vocabulary. Indices start from 1 since 0 is reserved for padding.\n    \"\"\"\n    word_to_id_dict: dict[str, int] = {}\n    id_to_word_dict: dict[int, str] = {}\n    vocab_size = 0\n        \n    def __init__(self, vocabulary: list[str], start_word: str = \"<start>\", stop_word: str = \"<stop>\", count_padding_as_separate_word: bool = True, padding: str = \"<padding>\"):\n        \"\"\"\n        ID 0 is used for padding\n        \n        Parameters\n        ----------\n        vocabulary: The list of words\n        start_word: special word for denoting the start of generation\n        stop_word: special word for denoting the stop of generation\n        count_padding_as_separate_word: if false, there won't be a entry for ID 0\n        padding: the special word place where id=0\n        \"\"\"\n        \n        self.start_word: str = start_word\n        self.stop_word: str = stop_word\n        \n        if count_padding_as_separate_word:\n            self.padding = padding\n            self.word_to_id_dict[self.padding] = 0\n            self.id_to_word_dict[0] = self.padding\n        \n        # adding start word in the vocabulary\n        self.word_to_id_dict[self.start_word] = 1\n        self.id_to_word_dict[1] = self.start_word\n        \n        last_index = 0\n        for idx, word in enumerate(vocabulary):\n            self.word_to_id_dict[word] = idx + 2\n            self.id_to_word_dict[idx + 2] = word \n            last_index = idx + 2\n            \n        # adding start word in the vocabulary\n        self.word_to_id_dict[self.stop_word] = last_index + 1\n        self.id_to_word_dict[last_index + 1] = self.stop_word\n        \n        assert len(self.word_to_id_dict) == len(self.id_to_word_dict)\n        \n        self.vocab_size = len(self.word_to_id_dict)\n\n    def id_of(self, word: str) -> int | None:\n        return self.word_to_id_dict[word]\n\n    def word_of(self, idx: int) -> str | None:\n        return self.id_to_word_dict[idx]\n    \n    def text_to_sequence(self, text: str, max_length: int = 0, padding: bool = False, pad_with: int = 0) -> np.ndarray:\n        \n        words = text.split()\n        \n        if not padding or max_length < 1:\n            if max_length < 1:\n                logger.error(f\"The provided maximum length {max_length} is invalid.\")\n            return np.array(list(map(lambda x: self.id_of(x), words)))\n        \n        len_words = len(words)\n        \n        padded_sequence = np.full(max_length, pad_with)\n        padded_sequence[:len_words] = np.array(list(map(lambda x: self.id_of(x), words)))\n        \n        \n        return padded_sequence\n    \n    def sequence_to_text(self, sequence: np.ndarray, padded: bool = False, padded_with: int = 0):\n        if not padded:\n            return \" \".join(map(lambda x: self.word_of(x), sequence))\n        \n        return \" \".join(filter(lambda y: y!=\"\", map(lambda x: self.word_of(x) if x!= padded_with else \"\", sequence)))\n    \n    \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:20.319170Z","iopub.execute_input":"2024-12-02T13:09:20.319471Z","iopub.status.idle":"2024-12-02T13:09:20.331665Z","shell.execute_reply.started":"2024-12-02T13:09:20.319444Z","shell.execute_reply":"2024-12-02T13:09:20.330740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"default_vocab_handler = VocabHandler(vocabulary)\nlogger.info(f\"Vocab Size: {default_vocab_handler.vocab_size}\")\nlogger.info(f\"Id of young is {default_vocab_handler.id_of('young')}\")\nlogger.info(f\"The word corresponding to id 12414 {default_vocab_handler.word_of(12414)}\") \n\nlogger.info(f\"Id of {default_vocab_handler.stop_word} is {default_vocab_handler.id_of(default_vocab_handler.stop_word)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:20.334021Z","iopub.execute_input":"2024-12-02T13:09:20.334271Z","iopub.status.idle":"2024-12-02T13:09:20.359663Z","shell.execute_reply.started":"2024-12-02T13:09:20.334245Z","shell.execute_reply":"2024-12-02T13:09:20.358863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# getting the maximum length of the words in a caption\n# this is important for padding the input as to provide equal length text input\nmaximum_length = max(captions_data[\"caption\"].apply(lambda caption: len(caption.split())))\nlogger.info(f\"The maximum number of words is {maximum_length}\")\n\nabsolute_max_length = maximum_length + 2 # including start and stop words\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:20.360775Z","iopub.execute_input":"2024-12-02T13:09:20.361103Z","iopub.status.idle":"2024-12-02T13:09:20.511136Z","shell.execute_reply.started":"2024-12-02T13:09:20.361066Z","shell.execute_reply":"2024-12-02T13:09:20.510496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Two young guys with shaggy hair look at their\nsample_text = \"<start> Two young guys with shaggy hair look at their <stop>\".lower()\noutput_sequence = default_vocab_handler.text_to_sequence(sample_text, absolute_max_length, padding = True)\noutput_text = default_vocab_handler.sequence_to_text(output_sequence, padded=True)\nlogger.info(f\"Input Text: {sample_text}\")\nlogger.info(f\"Output Sequence: {output_sequence}.\")\nlogger.info(f\"Output Text: {output_text}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:20.512146Z","iopub.execute_input":"2024-12-02T13:09:20.512792Z","iopub.status.idle":"2024-12-02T13:09:20.519908Z","shell.execute_reply.started":"2024-12-02T13:09:20.512753Z","shell.execute_reply":"2024-12-02T13:09:20.519109Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Generator","metadata":{}},{"cell_type":"code","source":"def data_generator(training_ids: list[str], vocab_handler: VocabHandler, max_length: int, batch_size: int):\n    \"\"\"\n    Generate infinite stream of batches\n\n    \n    \"\"\"\n    \n    vocab_size = vocab_handler.vocab_size\n    \n    while True:\n        img_inputs, text_inputs, text_outputs = list(), list(), list()\n        \n        random.shuffle(training_ids)\n        sample = 0\n        for img_id in training_ids:\n            img_path = images_dir.joinpath(img_id)\n            img = load_img(img_path, target_size=IMAGE_INPUT_SHAPE)\n            img_array = img_to_array(img)\n        \n            # get the captions corresponding to the image_id\n            captions = captions_data.loc[captions_data[\"image\"]==img_id]\n    \n            for caption in captions[\"caption\"].tolist():\n                words = caption.split()\n                words.insert(0, vocab_handler.start_word)\n                words.append(vocab_handler.stop_word)\n                n_words = len(words)\n    \n                for i in range(1, n_words):\n                    img_inputs.append(img_array)\n                    text_inputs.append(vocab_handler.text_to_sequence(\" \".join(words[:i]), max_length, True))\n                    text_outputs.append(to_categorical([vocab_handler.id_of(words[i])], num_classes=vocab_size)[0])\n        \n                    sample += 1\n            \n                    if sample == batch_size:\n                        sample = 0\n        \n                        img_inputs, text_inputs, text_outputs = preprocess_input(np.array(img_inputs)), np.array(text_inputs), np.array(text_outputs)\n        \n                        \n                        yield (img_inputs, text_inputs), text_outputs\n            \n                        img_inputs, text_inputs, text_outputs = list(), list(), list()\n                        \n        # Yield any remaining samples that didn't make a full batch\n        if img_inputs:\n            img_batch = preprocess_input(np.array(img_inputs))\n            text_batch = np.array(text_inputs)\n            output_batch = np.array(text_outputs)\n            \n            yield (img_batch, text_batch), output_batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:20.520915Z","iopub.execute_input":"2024-12-02T13:09:20.521330Z","iopub.status.idle":"2024-12-02T13:09:20.532424Z","shell.execute_reply.started":"2024-12-02T13:09:20.521304Z","shell.execute_reply":"2024-12-02T13:09:20.531592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_samples = 0\n_test_images = [\"1000092795.jpg\", \"10002456.jpg\", \"1000268201.jpg\", \"1000344755.jpg\", \"1000366164.jpg\", \"1000523639.jpg\", \"1000919630.jpg\", \"10010052.jpg\", \"1001465944.jpg\", \"1001545525.jpg\", \"1001573224.jpg\", \"1001633352.jpg\", \"1001773457.jpg\", \"1001896054.jpg\", \"100197432.jpg\", \"100207720.jpg\", \"1002674143.jpg\", \"1003163366.jpg\", \"1003420127.jpg\"]\nrandom.shuffle(_test_images)\n_data_generator = data_generator(_test_images,\n                  default_vocab_handler, absolute_max_length, 3)\n\nfor _ in range(n_samples):\n    (imgs, txt_input), text_outputs = _data_generator.__next__()\n\n    for img, txt_i, txt_o in zip(imgs, txt_input, text_outputs):        \n        print(f\"Input: {default_vocab_handler.sequence_to_text(txt_i, True)}\")\n        print(f\"To Predict: {default_vocab_handler.word_of(np.argmax(txt_o))}\")\n        plt.imshow(img)\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:20.533375Z","iopub.execute_input":"2024-12-02T13:09:20.533695Z","iopub.status.idle":"2024-12-02T13:09:20.544899Z","shell.execute_reply.started":"2024-12-02T13:09:20.533658Z","shell.execute_reply":"2024-12-02T13:09:20.544136Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model definition","metadata":{}},{"cell_type":"code","source":"def get_image_captioning_model():\n    # image feature extraction\n    image_input_layer = Input(shape=IMAGE_INPUT_SHAPE)\n    base_model = VGG19(\n        include_top=True, # include some fully connected layers too\n        weights=\"imagenet\",\n        input_tensor=None,\n        input_shape=IMAGE_INPUT_SHAPE,\n        pooling=\"max\",\n    )\n    base_model = Model(inputs=base_model.inputs, outputs=base_model.layers[-2].output) # discard the last two layers\n    base_model.trainable = TRAIN_CNN\n    feature_extraction_layers = base_model(image_input_layer, training=False)\n    # some trainable layers before merging\n    flatten = Flatten()(feature_extraction_layers)\n    feature_mapper = Dense(500)(flatten)\n    dropout_1 = Dropout(0.4)(feature_mapper)\n    image_feature_output = Dense(256, activation='relu')(dropout_1)\n    \n    # text feature extraction\n    text_input_layer = Input(shape=(absolute_max_length, ))\n    embed = Embedding(default_vocab_handler.vocab_size, 256, mask_zero=True)(text_input_layer)\n    dropout_2 = Dropout(0.4)(embed)\n    text_feature_output = LSTM(256)(dropout_2)\n    \n    # decoding\n    combine = add([image_feature_output, text_feature_output])\n    dense_decoder = Dense(256, activation='relu')(combine)\n    outputs = Dense(default_vocab_handler.vocab_size, activation='softmax')(dense_decoder)\n    \n    model = Model(inputs=[image_input_layer, text_input_layer], outputs=outputs)\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:20.545905Z","iopub.execute_input":"2024-12-02T13:09:20.546832Z","iopub.status.idle":"2024-12-02T13:09:20.554718Z","shell.execute_reply.started":"2024-12-02T13:09:20.546792Z","shell.execute_reply":"2024-12-02T13:09:20.554019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from contextlib import nullcontext\n\nif USE_MULTIPLE_GPUS:\n    logging.info(\"Using multiple GPUs with mirrored strategy.\")\n    strategy = tf.distribute.MirroredStrategy()\n    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n\n# Open a strategy scope if needed.\nwith strategy.scope() if USE_MULTIPLE_GPUS else nullcontext():\n\n    if LOAD_PRETRAINED:\n        logging.info(\"Loading the pretrained model.\")\n        model = tf.keras.models.load_model('/kaggle/input/image-caption-generator-dual-gpu-15-epochs/keras/default/1/models.keras')\n    else:\n        model = get_image_captioning_model()\n    \n        # compiling\n        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nplot_model(model, to_file=\"model.png\", show_shapes=True, show_layer_names=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:20.555766Z","iopub.execute_input":"2024-12-02T13:09:20.556604Z","iopub.status.idle":"2024-12-02T13:09:27.755801Z","shell.execute_reply.started":"2024-12-02T13:09:20.556566Z","shell.execute_reply":"2024-12-02T13:09:27.754989Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"markdown","source":"## Setting Up Tensorboard","metadata":{}},{"cell_type":"code","source":"logdir = \"/kaggle/working/logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:27.756984Z","iopub.execute_input":"2024-12-02T13:09:27.757237Z","iopub.status.idle":"2024-12-02T13:09:27.761280Z","shell.execute_reply.started":"2024-12-02T13:09:27.757212Z","shell.execute_reply":"2024-12-02T13:09:27.760429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# doesn't work on kaggle\n\n# # Load the TensorBoard notebook extension.\n# %load_ext tensorboard\n\n# %tensorboard --logdir /kaggle/working/logs/scalars","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:27.762255Z","iopub.execute_input":"2024-12-02T13:09:27.762564Z","iopub.status.idle":"2024-12-02T13:09:27.772827Z","shell.execute_reply.started":"2024-12-02T13:09:27.762530Z","shell.execute_reply":"2024-12-02T13:09:27.772033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setting Up Wandb","metadata":{}},{"cell_type":"code","source":"import wandb\nfrom wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint, WandbCallback\n\nwandb.login(key=\"bed61d44fc0bb5ea4949f930c43ce0f44dcc764a\")\n\n# Initialize a new W&B run\nwandb.init(config={\"bs\": 12}, project=\"Image Caption Generator-Multi-GPU\")\n\nmetric_logger_callback = WandbMetricsLogger(log_freq=\"batch\")\nmodel_save_callback = WandbModelCheckpoint(filepath=\"models.keras\", save_freq=\"epoch\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:27.773882Z","iopub.execute_input":"2024-12-02T13:09:27.774194Z","iopub.status.idle":"2024-12-02T13:09:32.914970Z","shell.execute_reply.started":"2024-12-02T13:09:27.774168Z","shell.execute_reply":"2024-12-02T13:09:32.914257Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Callback to clear memory and restart keras backend at the end of each epoch\nhttps://stackoverflow.com/questions/53683164/keras-occupies-an-indefinitely-increasing-amount-of-memory-for-each-epoch","metadata":{}},{"cell_type":"code","source":"import gc\nfrom tensorflow.keras import backend as k\nfrom tensorflow.keras.callbacks import Callback\n\nclass ClearMemory(Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:32.915715Z","iopub.execute_input":"2024-12-02T13:09:32.915958Z","iopub.status.idle":"2024-12-02T13:09:32.921217Z","shell.execute_reply.started":"2024-12-02T13:09:32.915932Z","shell.execute_reply":"2024-12-02T13:09:32.920329Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"all_img_ids = captions_data[\"image\"].unique().tolist()\n\nlogger.info(f\"Total samples: {len(all_img_ids)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:32.922259Z","iopub.execute_input":"2024-12-02T13:09:32.922519Z","iopub.status.idle":"2024-12-02T13:09:32.942139Z","shell.execute_reply.started":"2024-12-02T13:09:32.922494Z","shell.execute_reply":"2024-12-02T13:09:32.941336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_total_samples(data):\n    # Calculate the length of captions\n    length_of_caption_df = captions_data.copy()\n    length_of_caption_df[\"length_of_caption\"] = captions_data[\"caption\"].apply(lambda x: len(str(x).split()))\n    \n    # Filter the dataframe to only include images in the given data\n    filtered_df = length_of_caption_df[length_of_caption_df[\"image\"].isin(data)]\n    \n    # Sum the lengths directly\n    total_samples = filtered_df[\"length_of_caption\"].sum()\n    \n    return total_samples\n\ncalculate_total_samples([\"1000092795.jpg\", \"1000366164.jpg\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:32.943114Z","iopub.execute_input":"2024-12-02T13:09:32.943375Z","iopub.status.idle":"2024-12-02T13:09:33.110055Z","shell.execute_reply.started":"2024-12-02T13:09:32.943330Z","shell.execute_reply":"2024-12-02T13:09:33.109225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train, test_and_validation = train_test_split(all_img_ids, test_size=((TRAIN_TEST_VAL_SPLIT[1] + TRAIN_TEST_VAL_SPLIT[2])/ sum(TRAIN_TEST_VAL_SPLIT)))\ntest, validation = train_test_split(test_and_validation, test_size=TRAIN_TEST_VAL_SPLIT[2]/(TRAIN_TEST_VAL_SPLIT[1] + TRAIN_TEST_VAL_SPLIT[2]))\n\ntotal_training_samples = calculate_total_samples(train)\ntotal_validation_samples = calculate_total_samples(validation)\n\nsteps = total_training_samples // BATCH_SIZE\nval_steps = total_validation_samples // BATCH_SIZE\n\nlogger.info(f\"Total batches in an epoch: {steps}\")\nlogger.info(f\"Total batches in validation set: {val_steps}\")\n\ngenerator = data_generator(train, default_vocab_handler, absolute_max_length, BATCH_SIZE)\nval_generator = data_generator(validation, default_vocab_handler, absolute_max_length, BATCH_SIZE)\ntest_generator = data_generator(test, default_vocab_handler, absolute_max_length, BATCH_SIZE)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:33.111324Z","iopub.execute_input":"2024-12-02T13:09:33.111597Z","iopub.status.idle":"2024-12-02T13:09:33.438016Z","shell.execute_reply.started":"2024-12-02T13:09:33.111571Z","shell.execute_reply":"2024-12-02T13:09:33.437230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope() if USE_MULTIPLE_GPUS else nullcontext():\n    history = model.fit(\n        generator, \n        epochs=N_EPOCHS, \n        steps_per_epoch=steps, \n        verbose=1, validation_data=val_generator,\n        callbacks=[tensorboard_callback, metric_logger_callback, model_save_callback, ClearMemory()], \n        validation_freq=1, \n        validation_steps=val_steps)\n    if EVALUATE_AFTER_TRAIN:\n        model.evaluate(test_generator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T13:09:33.439028Z","iopub.execute_input":"2024-12-02T13:09:33.439282Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Saving and loading up the saved model to remove dependencies of multiple GPUS","metadata":{}},{"cell_type":"code","source":"# this is done because model trained using multiple-GPUS require more than one \n# sample in a batch to distribute it equally\n# since this is a bit complicated to run inference on multiple image,text pair\n# so, using this hack for now\nmodel.save('temp_model.keras')\n\nmodel = tf.keras.models.load_model('temp_model.keras')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing and visualization on few images","metadata":{}},{"cell_type":"code","source":"n_images_to_test = 2\n\ntest_images = random.sample(test, n_images_to_test)\n    \nfor image_id in test_images:    \n    img_path = images_dir.joinpath(image_id)\n    image = load_img(img_path, target_size=IMAGE_INPUT_SHAPE)\n    image = img_to_array(image)\n\n    reshaped_img = image.reshape(1, *IMAGE_INPUT_SHAPE)\n    image_input = preprocess_input(reshaped_img)\n    \n    text_input = \"<start> \"\n    whole_text_output = \"\"\n    for i in range(absolute_max_length):\n        sequence_input = default_vocab_handler.text_to_sequence(text_input, absolute_max_length, True)\n\n        model_input = [image_input, sequence_input.reshape((1,absolute_max_length))]\n        \n        sequence_output = np.argmax(model.predict(model_input))\n        \n        text_output = default_vocab_handler.word_of(sequence_output)\n        \n        if text_output == default_vocab_handler.stop_word:\n            break\n        whole_text_output += \" \" + text_output\n        text_input += \" \" + text_output\n    \n    print(f\"Generated: {whole_text_output}\")\n    print(f\"Actual: {captions_data.loc[captions_data['image'] == image_id]['caption'].tolist()}\")\n    plt.imshow(load_img(img_path, target_size=IMAGE_INPUT_SHAPE))\n    plt.show()\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing BLEU Score","metadata":{}},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nfrom tqdm import tqdm\n\nactual = []\npredicted = []\n    \nfor image_id in tqdm(test):\n    img_path = images_dir.joinpath(image_id)\n    image = load_img(img_path, target_size=IMAGE_INPUT_SHAPE)\n    image = img_to_array(image)\n\n    reshaped_img = image.reshape(1, *IMAGE_INPUT_SHAPE)\n    image_input = preprocess_input(reshaped_img)\n    \n    text_input = \"<start>\"\n    whole_text_output = \"\"\n    for i in range(absolute_max_length):\n        sequence_input = default_vocab_handler.text_to_sequence(text_input, absolute_max_length, True)\n        \n        sequence_output = np.argmax(model.predict([np.array(image_input), np.array(sequence_input.reshape((1,absolute_max_length)))], verbose=0))\n        \n        text_output = default_vocab_handler.word_of(sequence_output)\n        \n        if text_output == default_vocab_handler.stop_word:\n            break\n        whole_text_output += \" \" + text_output\n        text_input += \" \" + text_output\n\n    actual.append(captions_data.loc[captions_data['image'] == image_id]['caption'].tolist())\n    predicted.append(whole_text_output)\n    \nsentence_bleu(actual, predicted)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}