{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T11:51:22.890505Z",
     "iopub.status.busy": "2024-12-01T11:51:22.890154Z",
     "iopub.status.idle": "2024-12-01T11:51:23.941063Z",
     "shell.execute_reply": "2024-12-01T11:51:23.939845Z",
     "shell.execute_reply.started": "2024-12-01T11:51:22.890472Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls /kaggle/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow keras tensorboard scikit-learn ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:08:59.901966Z",
     "iopub.status.busy": "2024-12-02T13:08:59.901606Z",
     "iopub.status.idle": "2024-12-02T13:09:11.395276Z",
     "shell.execute_reply": "2024-12-02T13:09:11.394604Z",
     "shell.execute_reply.started": "2024-12-02T13:08:59.901926Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import random\n",
    "from PIL import Image \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, LSTM, add, Flatten\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# notebook specific\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:11.396980Z",
     "iopub.status.busy": "2024-12-02T13:09:11.396557Z",
     "iopub.status.idle": "2024-12-02T13:09:11.402377Z",
     "shell.execute_reply": "2024-12-02T13:09:11.401379Z",
     "shell.execute_reply.started": "2024-12-02T13:09:11.396952Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "TRAIN_CNN: bool = False\n",
    "NUM_OUTPUT_CAPTIONS: int = 1\n",
    "IMAGE_INPUT_SHAPE: tuple[int, int, int] = (224, 224, 3) # (height, width, channel)\n",
    "DATASET_NAME: str = \"flickr30k\"\n",
    "\n",
    "FILTER_NON_ALPHA_NUMERIC_STRINGS = True\n",
    "\n",
    "EVALUATE_AFTER_TRAIN: bool = False\n",
    "LOAD_PRETRAINED: bool = False # make it possible to continue training from a saved model\n",
    "USE_MULTIPLE_GPUS: bool = True \n",
    "\n",
    "# Hyperparameters\n",
    "N_EPOCHS: int = 5\n",
    "BATCH_SIZE: int = 64\n",
    "DEBUG: bool = False\n",
    "TRAIN_TEST_VAL_SPLIT = (70, 20, 10)\n",
    "    \n",
    "# paths\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "DATASET_INPUT_DIR = Path(\"/kaggle/input/\").joinpath(DATASET_NAME)\n",
    "TEMP_DIR = Path(\"/tmp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:11.403949Z",
     "iopub.status.busy": "2024-12-02T13:09:11.403598Z",
     "iopub.status.idle": "2024-12-02T13:09:11.480428Z",
     "shell.execute_reply": "2024-12-02T13:09:11.479611Z",
     "shell.execute_reply.started": "2024-12-02T13:09:11.403906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# setting up the logger\n",
    "logging.basicConfig(level=logging.DEBUG if DEBUG else logging.INFO, force=True) # a workaround\n",
    "logger = logging.getLogger(\"-^-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:11.482221Z",
     "iopub.status.busy": "2024-12-02T13:09:11.481981Z",
     "iopub.status.idle": "2024-12-02T13:09:11.504314Z",
     "shell.execute_reply": "2024-12-02T13:09:11.503584Z",
     "shell.execute_reply.started": "2024-12-02T13:09:11.482197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_good = True\n",
    "\n",
    "if not os.path.exists(WORKING_DIR):\n",
    "    logger.error(f\"The directory {WORKING_DIR} doesn't exist.\")\n",
    "    all_good = False\n",
    "\n",
    "if not os.path.exists(DATASET_INPUT_DIR):\n",
    "    logger.error(f\"The directory {DATASET_INPUT_DIR} doesn't exist.\")\n",
    "    all_good = False\n",
    "\n",
    "if not os.path.exists(TEMP_DIR):\n",
    "    logger.error(f\"The directory {TEMP_DIR} doesn't exist.\")\n",
    "    all_good = False\n",
    "\n",
    "if all_good:\n",
    "    logger.info(f\"All the directories are valid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:11.505465Z",
     "iopub.status.busy": "2024-12-02T13:09:11.505226Z",
     "iopub.status.idle": "2024-12-02T13:09:11.509036Z",
     "shell.execute_reply": "2024-12-02T13:09:11.508275Z",
     "shell.execute_reply.started": "2024-12-02T13:09:11.505441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images_dir = DATASET_INPUT_DIR.joinpath(\"Images\")\n",
    "captions_file = DATASET_INPUT_DIR.joinpath(\"captions.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:11.510483Z",
     "iopub.status.busy": "2024-12-02T13:09:11.510052Z",
     "iopub.status.idle": "2024-12-02T13:09:11.900429Z",
     "shell.execute_reply": "2024-12-02T13:09:11.899551Z",
     "shell.execute_reply.started": "2024-12-02T13:09:11.510443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "captions_data = pd.read_csv(captions_file)\n",
    "captions_data.astype(str)\n",
    "captions_data.dropna(inplace=True)\n",
    "logger.info(captions_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:11.901679Z",
     "iopub.status.busy": "2024-12-02T13:09:11.901428Z",
     "iopub.status.idle": "2024-12-02T13:09:12.149194Z",
     "shell.execute_reply": "2024-12-02T13:09:12.148399Z",
     "shell.execute_reply.started": "2024-12-02T13:09:11.901652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# filtering the columns having too few or too many words\n",
    "# as having too few and too many can improperly skew whole training process\n",
    "# having too many causes the whole network to be trained mostly on padding rather than the actual data\n",
    "\n",
    "upper_limit = 30\n",
    "lower_limit = 6\n",
    "\n",
    "captions_data = captions_data.drop(captions_data.loc[captions_data[\"caption\"].apply(lambda x: len(str(x).split()) > upper_limit or len(str(x).split()) < lower_limit)].index)\n",
    "\n",
    "len(captions_data[\"image\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:12.150708Z",
     "iopub.status.busy": "2024-12-02T13:09:12.150338Z",
     "iopub.status.idle": "2024-12-02T13:09:20.317727Z",
     "shell.execute_reply": "2024-12-02T13:09:20.316873Z",
     "shell.execute_reply.started": "2024-12-02T13:09:12.150669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unfiltered_vocabulary = list(set(\" \".join(captions_data[\"caption\"].to_list()).lower().split()))\n",
    "\n",
    "removed_items = []\n",
    "\n",
    "if FILTER_NON_ALPHA_NUMERIC_STRINGS:\n",
    "    vocabulary = list(filter(lambda x: len(x) >= 3 or x.isalpha() , unfiltered_vocabulary))\n",
    "    removed_items += list(filter(lambda x: len(x) < 3 and not x.isalpha() , unfiltered_vocabulary))\n",
    "else:\n",
    "    vocabulary = unfiltered_vocabulary\n",
    "\n",
    "filtered_captions_data = captions_data.copy()\n",
    "filtered_captions_data[\"caption\"] = filtered_captions_data[\"caption\"].apply(\n",
    "    lambda x: \" \".join(list(filter(\n",
    "        lambda y: y not in removed_items, x.lower().split()))\n",
    "                      )\n",
    ")\n",
    "\n",
    "vocabulary_from_filtered_captions_data = list(set(\" \".join(filtered_captions_data[\"caption\"].to_list()).lower().split()))\n",
    "\n",
    "for x in vocabulary_from_filtered_captions_data:\n",
    "    if x not in vocabulary:\n",
    "        logger.error(f\"Word: '{x}' not in vocabulary\")\n",
    "        raise Exception(\"Found a word that is not in the vocabulary\")\n",
    "\n",
    "captions_data = filtered_captions_data\n",
    "\n",
    "logger.info(f\"Removed items: {removed_items}\")\n",
    "\n",
    "logger.info(f\"Total unique words: {len(vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:20.319471Z",
     "iopub.status.busy": "2024-12-02T13:09:20.319170Z",
     "iopub.status.idle": "2024-12-02T13:09:20.331665Z",
     "shell.execute_reply": "2024-12-02T13:09:20.330740Z",
     "shell.execute_reply.started": "2024-12-02T13:09:20.319444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VocabHandler():\n",
    "    \"\"\"\n",
    "    Handles vocabulary. Indices start from 1 since 0 is reserved for padding.\n",
    "    \"\"\"\n",
    "    word_to_id_dict: dict[str, int] = {}\n",
    "    id_to_word_dict: dict[int, str] = {}\n",
    "    vocab_size = 0\n",
    "        \n",
    "    def __init__(self, vocabulary: list[str], start_word: str = \"<start>\", stop_word: str = \"<stop>\", count_padding_as_separate_word: bool = True, padding: str = \"<padding>\"):\n",
    "        \"\"\"\n",
    "        ID 0 is used for padding\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        vocabulary: The list of words\n",
    "        start_word: special word for denoting the start of generation\n",
    "        stop_word: special word for denoting the stop of generation\n",
    "        count_padding_as_separate_word: if false, there won't be a entry for ID 0\n",
    "        padding: the special word place where id=0\n",
    "        \"\"\"\n",
    "        \n",
    "        self.start_word: str = start_word\n",
    "        self.stop_word: str = stop_word\n",
    "        \n",
    "        if count_padding_as_separate_word:\n",
    "            self.padding = padding\n",
    "            self.word_to_id_dict[self.padding] = 0\n",
    "            self.id_to_word_dict[0] = self.padding\n",
    "        \n",
    "        # adding start word in the vocabulary\n",
    "        self.word_to_id_dict[self.start_word] = 1\n",
    "        self.id_to_word_dict[1] = self.start_word\n",
    "        \n",
    "        last_index = 0\n",
    "        for idx, word in enumerate(vocabulary):\n",
    "            self.word_to_id_dict[word] = idx + 2\n",
    "            self.id_to_word_dict[idx + 2] = word \n",
    "            last_index = idx + 2\n",
    "            \n",
    "        # adding start word in the vocabulary\n",
    "        self.word_to_id_dict[self.stop_word] = last_index + 1\n",
    "        self.id_to_word_dict[last_index + 1] = self.stop_word\n",
    "        \n",
    "        assert len(self.word_to_id_dict) == len(self.id_to_word_dict)\n",
    "        \n",
    "        self.vocab_size = len(self.word_to_id_dict)\n",
    "\n",
    "    def id_of(self, word: str) -> int | None:\n",
    "        return self.word_to_id_dict[word]\n",
    "\n",
    "    def word_of(self, idx: int) -> str | None:\n",
    "        return self.id_to_word_dict[idx]\n",
    "    \n",
    "    def text_to_sequence(self, text: str, max_length: int = 0, padding: bool = False, pad_with: int = 0) -> np.ndarray:\n",
    "        \n",
    "        words = text.split()\n",
    "        \n",
    "        if not padding or max_length < 1:\n",
    "            if max_length < 1:\n",
    "                logger.error(f\"The provided maximum length {max_length} is invalid.\")\n",
    "            return np.array(list(map(lambda x: self.id_of(x), words)))\n",
    "        \n",
    "        len_words = len(words)\n",
    "        \n",
    "        padded_sequence = np.full(max_length, pad_with)\n",
    "        padded_sequence[:len_words] = np.array(list(map(lambda x: self.id_of(x), words)))\n",
    "        \n",
    "        \n",
    "        return padded_sequence\n",
    "    \n",
    "    def sequence_to_text(self, sequence: np.ndarray, padded: bool = False, padded_with: int = 0):\n",
    "        if not padded:\n",
    "            return \" \".join(map(lambda x: self.word_of(x), sequence))\n",
    "        \n",
    "        return \" \".join(filter(lambda y: y!=\"\", map(lambda x: self.word_of(x) if x!= padded_with else \"\", sequence)))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:20.334271Z",
     "iopub.status.busy": "2024-12-02T13:09:20.334021Z",
     "iopub.status.idle": "2024-12-02T13:09:20.359663Z",
     "shell.execute_reply": "2024-12-02T13:09:20.358863Z",
     "shell.execute_reply.started": "2024-12-02T13:09:20.334245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "default_vocab_handler = VocabHandler(vocabulary)\n",
    "logger.info(f\"Vocab Size: {default_vocab_handler.vocab_size}\")\n",
    "logger.info(f\"Id of young is {default_vocab_handler.id_of('young')}\")\n",
    "logger.info(f\"The word corresponding to id 12414 {default_vocab_handler.word_of(12414)}\") \n",
    "\n",
    "logger.info(f\"Id of {default_vocab_handler.stop_word} is {default_vocab_handler.id_of(default_vocab_handler.stop_word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:20.361103Z",
     "iopub.status.busy": "2024-12-02T13:09:20.360775Z",
     "iopub.status.idle": "2024-12-02T13:09:20.511136Z",
     "shell.execute_reply": "2024-12-02T13:09:20.510496Z",
     "shell.execute_reply.started": "2024-12-02T13:09:20.361066Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# getting the maximum length of the words in a caption\n",
    "# this is important for padding the input as to provide equal length text input\n",
    "maximum_length = max(captions_data[\"caption\"].apply(lambda caption: len(caption.split())))\n",
    "logger.info(f\"The maximum number of words is {maximum_length}\")\n",
    "\n",
    "absolute_max_length = maximum_length + 2 # including start and stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:20.512792Z",
     "iopub.status.busy": "2024-12-02T13:09:20.512146Z",
     "iopub.status.idle": "2024-12-02T13:09:20.519908Z",
     "shell.execute_reply": "2024-12-02T13:09:20.519109Z",
     "shell.execute_reply.started": "2024-12-02T13:09:20.512753Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Two young guys with shaggy hair look at their\n",
    "sample_text = \"<start> Two young guys with shaggy hair look at their <stop>\".lower()\n",
    "output_sequence = default_vocab_handler.text_to_sequence(sample_text, absolute_max_length, padding = True)\n",
    "output_text = default_vocab_handler.sequence_to_text(output_sequence, padded=True)\n",
    "logger.info(f\"Input Text: {sample_text}\")\n",
    "logger.info(f\"Output Sequence: {output_sequence}.\")\n",
    "logger.info(f\"Output Text: {output_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:20.521330Z",
     "iopub.status.busy": "2024-12-02T13:09:20.520915Z",
     "iopub.status.idle": "2024-12-02T13:09:20.532424Z",
     "shell.execute_reply": "2024-12-02T13:09:20.531592Z",
     "shell.execute_reply.started": "2024-12-02T13:09:20.521304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def data_generator(training_ids: list[str], vocab_handler: VocabHandler, max_length: int, batch_size: int):\n",
    "    \"\"\"\n",
    "    Generate infinite stream of batches\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_size = vocab_handler.vocab_size\n",
    "    \n",
    "    while True:\n",
    "        img_inputs, text_inputs, text_outputs = list(), list(), list()\n",
    "        \n",
    "        random.shuffle(training_ids)\n",
    "        sample = 0\n",
    "        for img_id in training_ids:\n",
    "            img_path = images_dir.joinpath(img_id)\n",
    "            img = load_img(img_path, target_size=IMAGE_INPUT_SHAPE)\n",
    "            img_array = img_to_array(img)\n",
    "        \n",
    "            # get the captions corresponding to the image_id\n",
    "            captions = captions_data.loc[captions_data[\"image\"]==img_id]\n",
    "    \n",
    "            for caption in captions[\"caption\"].tolist():\n",
    "                words = caption.split()\n",
    "                words.insert(0, vocab_handler.start_word)\n",
    "                words.append(vocab_handler.stop_word)\n",
    "                n_words = len(words)\n",
    "    \n",
    "                for i in range(1, n_words):\n",
    "                    img_inputs.append(img_array)\n",
    "                    text_inputs.append(vocab_handler.text_to_sequence(\" \".join(words[:i]), max_length, True))\n",
    "                    text_outputs.append(to_categorical([vocab_handler.id_of(words[i])], num_classes=vocab_size)[0])\n",
    "        \n",
    "                    sample += 1\n",
    "            \n",
    "                    if sample == batch_size:\n",
    "                        sample = 0\n",
    "        \n",
    "                        img_inputs, text_inputs, text_outputs = preprocess_input(np.array(img_inputs)), np.array(text_inputs), np.array(text_outputs)\n",
    "        \n",
    "                        \n",
    "                        yield (img_inputs, text_inputs), text_outputs\n",
    "            \n",
    "                        img_inputs, text_inputs, text_outputs = list(), list(), list()\n",
    "                        \n",
    "        # Yield any remaining samples that didn't make a full batch\n",
    "        if img_inputs:\n",
    "            img_batch = preprocess_input(np.array(img_inputs))\n",
    "            text_batch = np.array(text_inputs)\n",
    "            output_batch = np.array(text_outputs)\n",
    "            \n",
    "            yield (img_batch, text_batch), output_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:20.533695Z",
     "iopub.status.busy": "2024-12-02T13:09:20.533375Z",
     "iopub.status.idle": "2024-12-02T13:09:20.544899Z",
     "shell.execute_reply": "2024-12-02T13:09:20.544136Z",
     "shell.execute_reply.started": "2024-12-02T13:09:20.533658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_samples = 0\n",
    "_test_images = [\"1000092795.jpg\", \"10002456.jpg\", \"1000268201.jpg\", \"1000344755.jpg\", \"1000366164.jpg\", \"1000523639.jpg\", \"1000919630.jpg\", \"10010052.jpg\", \"1001465944.jpg\", \"1001545525.jpg\", \"1001573224.jpg\", \"1001633352.jpg\", \"1001773457.jpg\", \"1001896054.jpg\", \"100197432.jpg\", \"100207720.jpg\", \"1002674143.jpg\", \"1003163366.jpg\", \"1003420127.jpg\"]\n",
    "random.shuffle(_test_images)\n",
    "_data_generator = data_generator(_test_images,\n",
    "                  default_vocab_handler, absolute_max_length, 3)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    (imgs, txt_input), text_outputs = _data_generator.__next__()\n",
    "\n",
    "    for img, txt_i, txt_o in zip(imgs, txt_input, text_outputs):        \n",
    "        print(f\"Input: {default_vocab_handler.sequence_to_text(txt_i, True)}\")\n",
    "        print(f\"To Predict: {default_vocab_handler.word_of(np.argmax(txt_o))}\")\n",
    "        plt.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:20.546832Z",
     "iopub.status.busy": "2024-12-02T13:09:20.545905Z",
     "iopub.status.idle": "2024-12-02T13:09:20.554718Z",
     "shell.execute_reply": "2024-12-02T13:09:20.554019Z",
     "shell.execute_reply.started": "2024-12-02T13:09:20.546792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_image_captioning_model():\n",
    "    # image feature extraction\n",
    "    image_input_layer = Input(shape=IMAGE_INPUT_SHAPE)\n",
    "    base_model = VGG19(\n",
    "        include_top=True, # include some fully connected layers too\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=None,\n",
    "        input_shape=IMAGE_INPUT_SHAPE,\n",
    "        pooling=\"max\",\n",
    "    )\n",
    "    base_model = Model(inputs=base_model.inputs, outputs=base_model.layers[-2].output) # discard the last two layers\n",
    "    base_model.trainable = TRAIN_CNN\n",
    "    feature_extraction_layers = base_model(image_input_layer, training=False)\n",
    "    # some trainable layers before merging\n",
    "    flatten = Flatten()(feature_extraction_layers)\n",
    "    feature_mapper = Dense(500)(flatten)\n",
    "    dropout_1 = Dropout(0.4)(feature_mapper)\n",
    "    image_feature_output = Dense(256, activation='relu')(dropout_1)\n",
    "    \n",
    "    # text feature extraction\n",
    "    text_input_layer = Input(shape=(absolute_max_length, ))\n",
    "    embed = Embedding(default_vocab_handler.vocab_size, 256, mask_zero=True)(text_input_layer)\n",
    "    dropout_2 = Dropout(0.4)(embed)\n",
    "    text_feature_output = LSTM(256)(dropout_2)\n",
    "    \n",
    "    # decoding\n",
    "    combine = add([image_feature_output, text_feature_output])\n",
    "    dense_decoder = Dense(256, activation='relu')(combine)\n",
    "    outputs = Dense(default_vocab_handler.vocab_size, activation='softmax')(dense_decoder)\n",
    "    \n",
    "    model = Model(inputs=[image_input_layer, text_input_layer], outputs=outputs)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:20.556604Z",
     "iopub.status.busy": "2024-12-02T13:09:20.555766Z",
     "iopub.status.idle": "2024-12-02T13:09:27.755801Z",
     "shell.execute_reply": "2024-12-02T13:09:27.754989Z",
     "shell.execute_reply.started": "2024-12-02T13:09:20.556566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "\n",
    "if USE_MULTIPLE_GPUS:\n",
    "    logging.info(\"Using multiple GPUs with mirrored strategy.\")\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope if needed.\n",
    "with strategy.scope() if USE_MULTIPLE_GPUS else nullcontext():\n",
    "\n",
    "    if LOAD_PRETRAINED:\n",
    "        logging.info(\"Loading the pretrained model.\")\n",
    "        model = tf.keras.models.load_model('/kaggle/input/image-caption-generator-dual-gpu-15-epochs/keras/default/1/models.keras')\n",
    "    else:\n",
    "        model = get_image_captioning_model()\n",
    "    \n",
    "        # compiling\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "plot_model(model, to_file=\"model.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:27.757237Z",
     "iopub.status.busy": "2024-12-02T13:09:27.756984Z",
     "iopub.status.idle": "2024-12-02T13:09:27.761280Z",
     "shell.execute_reply": "2024-12-02T13:09:27.760429Z",
     "shell.execute_reply.started": "2024-12-02T13:09:27.757212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "logdir = \"/kaggle/working/logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:27.762564Z",
     "iopub.status.busy": "2024-12-02T13:09:27.762255Z",
     "iopub.status.idle": "2024-12-02T13:09:27.772827Z",
     "shell.execute_reply": "2024-12-02T13:09:27.772033Z",
     "shell.execute_reply.started": "2024-12-02T13:09:27.762530Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# doesn't work on kaggle\n",
    "\n",
    "# # Load the TensorBoard notebook extension.\n",
    "# %load_ext tensorboard\n",
    "\n",
    "# %tensorboard --logdir /kaggle/working/logs/scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:27.774194Z",
     "iopub.status.busy": "2024-12-02T13:09:27.773882Z",
     "iopub.status.idle": "2024-12-02T13:09:32.914970Z",
     "shell.execute_reply": "2024-12-02T13:09:32.914257Z",
     "shell.execute_reply.started": "2024-12-02T13:09:27.774168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint, WandbCallback\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "# Initialize a new W&B run\n",
    "wandb.init(config={\"bs\": 12}, project=\"Image Caption Generator-Multi-GPU\")\n",
    "\n",
    "metric_logger_callback = WandbMetricsLogger(log_freq=\"batch\")\n",
    "model_save_callback = WandbModelCheckpoint(filepath=\"models.keras\", save_freq=\"epoch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback to clear memory and restart keras backend at the end of each epoch\n",
    "https://stackoverflow.com/questions/53683164/keras-occupies-an-indefinitely-increasing-amount-of-memory-for-each-epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:32.915958Z",
     "iopub.status.busy": "2024-12-02T13:09:32.915715Z",
     "iopub.status.idle": "2024-12-02T13:09:32.921217Z",
     "shell.execute_reply": "2024-12-02T13:09:32.920329Z",
     "shell.execute_reply.started": "2024-12-02T13:09:32.915932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class ClearMemory(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:32.922519Z",
     "iopub.status.busy": "2024-12-02T13:09:32.922259Z",
     "iopub.status.idle": "2024-12-02T13:09:32.942139Z",
     "shell.execute_reply": "2024-12-02T13:09:32.941336Z",
     "shell.execute_reply.started": "2024-12-02T13:09:32.922494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_img_ids = captions_data[\"image\"].unique().tolist()\n",
    "\n",
    "logger.info(f\"Total samples: {len(all_img_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:32.943375Z",
     "iopub.status.busy": "2024-12-02T13:09:32.943114Z",
     "iopub.status.idle": "2024-12-02T13:09:33.110055Z",
     "shell.execute_reply": "2024-12-02T13:09:33.109225Z",
     "shell.execute_reply.started": "2024-12-02T13:09:32.943330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_total_samples(data):\n",
    "    # Calculate the length of captions\n",
    "    length_of_caption_df = captions_data.copy()\n",
    "    length_of_caption_df[\"length_of_caption\"] = captions_data[\"caption\"].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Filter the dataframe to only include images in the given data\n",
    "    filtered_df = length_of_caption_df[length_of_caption_df[\"image\"].isin(data)]\n",
    "    \n",
    "    # Sum the lengths directly\n",
    "    total_samples = filtered_df[\"length_of_caption\"].sum()\n",
    "    \n",
    "    return total_samples\n",
    "\n",
    "calculate_total_samples([\"1000092795.jpg\", \"1000366164.jpg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:33.111597Z",
     "iopub.status.busy": "2024-12-02T13:09:33.111324Z",
     "iopub.status.idle": "2024-12-02T13:09:33.438016Z",
     "shell.execute_reply": "2024-12-02T13:09:33.437230Z",
     "shell.execute_reply.started": "2024-12-02T13:09:33.111571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train, test_and_validation = train_test_split(all_img_ids, test_size=((TRAIN_TEST_VAL_SPLIT[1] + TRAIN_TEST_VAL_SPLIT[2])/ sum(TRAIN_TEST_VAL_SPLIT)))\n",
    "test, validation = train_test_split(test_and_validation, test_size=TRAIN_TEST_VAL_SPLIT[2]/(TRAIN_TEST_VAL_SPLIT[1] + TRAIN_TEST_VAL_SPLIT[2]))\n",
    "\n",
    "total_training_samples = calculate_total_samples(train)\n",
    "total_validation_samples = calculate_total_samples(validation)\n",
    "\n",
    "steps = total_training_samples // BATCH_SIZE\n",
    "val_steps = total_validation_samples // BATCH_SIZE\n",
    "\n",
    "logger.info(f\"Total batches in an epoch: {steps}\")\n",
    "logger.info(f\"Total batches in validation set: {val_steps}\")\n",
    "\n",
    "generator = data_generator(train, default_vocab_handler, absolute_max_length, BATCH_SIZE)\n",
    "val_generator = data_generator(validation, default_vocab_handler, absolute_max_length, BATCH_SIZE)\n",
    "test_generator = data_generator(test, default_vocab_handler, absolute_max_length, BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T13:09:33.439282Z",
     "iopub.status.busy": "2024-12-02T13:09:33.439028Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with strategy.scope() if USE_MULTIPLE_GPUS else nullcontext():\n",
    "    history = model.fit(\n",
    "        generator, \n",
    "        epochs=N_EPOCHS, \n",
    "        steps_per_epoch=steps, \n",
    "        verbose=1, validation_data=val_generator,\n",
    "        callbacks=[tensorboard_callback, metric_logger_callback, model_save_callback, ClearMemory()], \n",
    "        validation_freq=1, \n",
    "        validation_steps=val_steps)\n",
    "    if EVALUATE_AFTER_TRAIN:\n",
    "        model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading up the saved model to remove dependencies of multiple GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# this is done because model trained using multiple-GPUS require more than one \n",
    "# sample in a batch to distribute it equally\n",
    "# since this is a bit complicated to run inference on multiple image,text pair\n",
    "# so, using this hack for now\n",
    "model.save('temp_model.keras')\n",
    "\n",
    "model = tf.keras.models.load_model('temp_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and visualization on few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_images_to_test = 2\n",
    "\n",
    "test_images = random.sample(test, n_images_to_test)\n",
    "    \n",
    "for image_id in test_images:    \n",
    "    img_path = images_dir.joinpath(image_id)\n",
    "    image = load_img(img_path, target_size=IMAGE_INPUT_SHAPE)\n",
    "    image = img_to_array(image)\n",
    "\n",
    "    reshaped_img = image.reshape(1, *IMAGE_INPUT_SHAPE)\n",
    "    image_input = preprocess_input(reshaped_img)\n",
    "    \n",
    "    text_input = \"<start> \"\n",
    "    whole_text_output = \"\"\n",
    "    for i in range(absolute_max_length):\n",
    "        sequence_input = default_vocab_handler.text_to_sequence(text_input, absolute_max_length, True)\n",
    "\n",
    "        model_input = [image_input, sequence_input.reshape((1,absolute_max_length))]\n",
    "        \n",
    "        sequence_output = np.argmax(model.predict(model_input))\n",
    "        \n",
    "        text_output = default_vocab_handler.word_of(sequence_output)\n",
    "        \n",
    "        if text_output == default_vocab_handler.stop_word:\n",
    "            break\n",
    "        whole_text_output += \" \" + text_output\n",
    "        text_input += \" \" + text_output\n",
    "    \n",
    "    print(f\"Generated: {whole_text_output}\")\n",
    "    print(f\"Actual: {captions_data.loc[captions_data['image'] == image_id]['caption'].tolist()}\")\n",
    "    plt.imshow(load_img(img_path, target_size=IMAGE_INPUT_SHAPE))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "actual = []\n",
    "predicted = []\n",
    "    \n",
    "for image_id in tqdm(test):\n",
    "    img_path = images_dir.joinpath(image_id)\n",
    "    image = load_img(img_path, target_size=IMAGE_INPUT_SHAPE)\n",
    "    image = img_to_array(image)\n",
    "\n",
    "    reshaped_img = image.reshape(1, *IMAGE_INPUT_SHAPE)\n",
    "    image_input = preprocess_input(reshaped_img)\n",
    "    \n",
    "    text_input = \"<start>\"\n",
    "    whole_text_output = \"\"\n",
    "    for i in range(absolute_max_length):\n",
    "        sequence_input = default_vocab_handler.text_to_sequence(text_input, absolute_max_length, True)\n",
    "        \n",
    "        sequence_output = np.argmax(model.predict([np.array(image_input), np.array(sequence_input.reshape((1,absolute_max_length)))], verbose=0))\n",
    "        \n",
    "        text_output = default_vocab_handler.word_of(sequence_output)\n",
    "        \n",
    "        if text_output == default_vocab_handler.stop_word:\n",
    "            break\n",
    "        whole_text_output += \" \" + text_output\n",
    "        text_input += \" \" + text_output\n",
    "\n",
    "    actual.append(captions_data.loc[captions_data['image'] == image_id]['caption'].tolist())\n",
    "    predicted.append(whole_text_output)\n",
    "    \n",
    "sentence_bleu(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 623329,
     "sourceId": 1111749,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
