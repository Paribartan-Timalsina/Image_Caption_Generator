{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111749,"sourceType":"datasetVersion","datasetId":623329},{"sourceId":10078191,"sourceType":"datasetVersion","datasetId":6212676}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting Up","metadata":{}},{"cell_type":"markdown","source":"## Installing requirements","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:59:35.716685Z","iopub.execute_input":"2024-12-04T14:59:35.717202Z","iopub.status.idle":"2024-12-04T14:59:36.898756Z","shell.execute_reply.started":"2024-12-04T14:59:35.717153Z","shell.execute_reply":"2024-12-04T14:59:36.897340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tensorflow keras tensorboard scikit-learn ipywidgets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T13:01:20.672463Z","iopub.execute_input":"2024-12-04T13:01:20.673357Z","iopub.status.idle":"2024-12-04T13:01:33.839498Z","shell.execute_reply.started":"2024-12-04T13:01:20.673295Z","shell.execute_reply":"2024-12-04T13:01:33.838138Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport os\nimport sys\n\nimport random\nfrom PIL import Image \n\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\nimport pickle\n\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport keras\nfrom keras.applications.vgg19 import VGG19, preprocess_input\nfrom keras.layers import Input, Dense, Dropout, Embedding, LSTM, add, Flatten\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.utils import to_categorical, plot_model\nfrom keras import Model\n\nfrom sklearn.model_selection import train_test_split\n\n# notebook specific\nfrom IPython.display import display\n# from tqdm.notebook import tqdm # for some reason this doesn't work\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:25:51.218750Z","iopub.execute_input":"2024-12-04T15:25:51.219119Z","iopub.status.idle":"2024-12-04T15:26:02.866217Z","shell.execute_reply.started":"2024-12-04T15:25:51.219085Z","shell.execute_reply":"2024-12-04T15:26:02.865491Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Some Parameters","metadata":{}},{"cell_type":"code","source":"# constants\nTRAIN_CNN: bool = False\nNUM_OUTPUT_CAPTIONS: int = 1\nIMAGE_INPUT_SHAPE: tuple[int, int, int] = (224, 224, 3) # (height, width, channel)\n\nDATASET_NAME: str = \"flickr30k\"\nFEATURES_NAME: str = \"flickr-31k-features-all\"\n\nFILTER_NON_ALPHA_NUMERIC_STRINGS = True\n\nEVALUATE_AFTER_TRAIN: bool = False\nLOAD_PRETRAINED: bool = False # make it possible to continue training from a saved model\nTAKE_FEATURES_FROM_INPUT: bool = True # load features from already saved file as infering this takes a long time\nCONTINUE_FROM_WANDB_RUN: bool = False # if needed to load pretrained model from wandb\nUSE_MULTIPLE_GPUS: bool = True \n\n# Hyperparameters\nN_EPOCHS: int = 35\nBATCH_SIZE: int = 1024\nDEBUG: bool = False\nTRAIN_TEST_VAL_SPLIT = (70, 20, 10)\n    \n# paths\nWORKING_DIR = Path(\"/kaggle/working\")\nINPUT_DIR = Path(\"/kaggle/input/\")\nDATASET_INPUT_DIR = INPUT_DIR.joinpath(DATASET_NAME)\nTEMP_DIR = Path(\"/tmp\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\n\n# setting up the logger\nlogging.basicConfig(level=logging.DEBUG if DEBUG else logging.INFO, force=True) # a workaround\nlogger = logging.getLogger(\"-^-\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_good = True\n\nif not os.path.exists(WORKING_DIR):\n    logger.error(f\"The directory {WORKING_DIR} doesn't exist.\")\n    all_good = False\n\nif not os.path.exists(DATASET_INPUT_DIR):\n    logger.error(f\"The directory {DATASET_INPUT_DIR} doesn't exist.\")\n    all_good = False\n\nif not os.path.exists(TEMP_DIR):\n    logger.error(f\"The directory {TEMP_DIR} doesn't exist.\")\n    all_good = False\n\nif all_good:\n    logger.info(f\"All the directories are valid.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Preparation","metadata":{}},{"cell_type":"code","source":"images_dir = DATASET_INPUT_DIR.joinpath(\"Images\")\ncaptions_file = DATASET_INPUT_DIR.joinpath(\"captions.txt\")\n\nif TAKE_FEATURES_FROM_INPUT:\n    features_dir = INPUT_DIR.joinpath(FEATURES_NAME)\nelse:\n    features_dir = WORKING_DIR.joinpath(\"features\")\n    \n    features_dir.mkdir(exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating the vocabulary","metadata":{}},{"cell_type":"code","source":"captions_data = pd.read_csv(captions_file)\ncaptions_data.astype(str)\ncaptions_data.dropna(inplace=True)\nlogger.info(captions_data.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# filtering the columns having too few or too many words\n# as having too few and too many can improperly skew whole training process\n# having too many causes the whole network to be trained mostly on padding rather than the actual data\n\nupper_limit = 30\nlower_limit = 6\n\ncaptions_data = captions_data.drop(captions_data.loc[captions_data[\"caption\"].apply(lambda x: len(str(x).split()) > upper_limit or len(str(x).split()) < lower_limit)].index)\n\nlen(captions_data[\"image\"].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unfiltered_vocabulary = list(set(\" \".join(captions_data[\"caption\"].to_list()).lower().split()))\n\nremoved_items = []\n\nif FILTER_NON_ALPHA_NUMERIC_STRINGS:\n    vocabulary = list(filter(lambda x: len(x) >= 3 or x.isalpha() , unfiltered_vocabulary))\n    removed_items += list(filter(lambda x: len(x) < 3 and not x.isalpha() , unfiltered_vocabulary))\nelse:\n    vocabulary = unfiltered_vocabulary\n\nfiltered_captions_data = captions_data.copy()\nfiltered_captions_data[\"caption\"] = filtered_captions_data[\"caption\"].apply(\n    lambda x: \" \".join(list(filter(\n        lambda y: y not in removed_items, x.lower().split()))\n                      )\n)\n\nvocabulary_from_filtered_captions_data = list(set(\" \".join(filtered_captions_data[\"caption\"].to_list()).lower().split()))\n\nfor x in vocabulary_from_filtered_captions_data:\n    if x not in vocabulary:\n        logger.error(f\"Word: '{x}' not in vocabulary\")\n        raise Exception(\"Found a word that is not in the vocabulary\")\n\ncaptions_data = filtered_captions_data\n\nlogger.info(f\"Removed items: {removed_items}\")\n\nlogger.info(f\"Total unique words: {len(vocabulary)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VocabHandler():\n    \"\"\"\n    Handles vocabulary. Indices start from 1 since 0 is reserved for padding.\n    \"\"\"\n    word_to_id_dict: dict[str, int] = {}\n    id_to_word_dict: dict[int, str] = {}\n    vocab_size = 0\n        \n    def __init__(self, vocabulary: list[str], start_word: str = \"<start>\", stop_word: str = \"<stop>\", count_padding_as_separate_word: bool = True, padding: str = \"<padding>\"):\n        \"\"\"\n        ID 0 is used for padding\n        \n        Parameters\n        ----------\n        vocabulary: The list of words\n        start_word: special word for denoting the start of generation\n        stop_word: special word for denoting the stop of generation\n        count_padding_as_separate_word: if false, there won't be a entry for ID 0\n        padding: the special word place where id=0\n        \"\"\"\n        \n        self.start_word: str = start_word\n        self.stop_word: str = stop_word\n        \n        if count_padding_as_separate_word:\n            self.padding = padding\n            self.word_to_id_dict[self.padding] = 0\n            self.id_to_word_dict[0] = self.padding\n        \n        # adding start word in the vocabulary\n        self.word_to_id_dict[self.start_word] = 1\n        self.id_to_word_dict[1] = self.start_word\n        \n        last_index = 0\n        for idx, word in enumerate(vocabulary):\n            self.word_to_id_dict[word] = idx + 2\n            self.id_to_word_dict[idx + 2] = word \n            last_index = idx + 2\n            \n        # adding start word in the vocabulary\n        self.word_to_id_dict[self.stop_word] = last_index + 1\n        self.id_to_word_dict[last_index + 1] = self.stop_word\n        \n        assert len(self.word_to_id_dict) == len(self.id_to_word_dict)\n        \n        self.vocab_size = len(self.word_to_id_dict)\n\n    def id_of(self, word: str) -> int | None:\n        return self.word_to_id_dict[word]\n\n    def word_of(self, idx: int) -> str | None:\n        return self.id_to_word_dict[idx]\n    \n    def text_to_sequence(self, text: str, max_length: int = 0, padding: bool = False, pad_with: int = 0) -> np.ndarray:\n        \n        words = text.split()\n        \n        if not padding or max_length < 1:\n            if max_length < 1:\n                logger.error(f\"The provided maximum length {max_length} is invalid.\")\n            return np.array(list(map(lambda x: self.id_of(x), words)))\n        \n        len_words = len(words)\n        \n        padded_sequence = np.full(max_length, pad_with)\n        padded_sequence[:len_words] = np.array(list(map(lambda x: self.id_of(x), words)))\n        \n        \n        return padded_sequence\n    \n    def sequence_to_text(self, sequence: np.ndarray, padded: bool = False, padded_with: int = 0):\n        if not padded:\n            return \" \".join(map(lambda x: self.word_of(x), sequence))\n        \n        return \" \".join(filter(lambda y: y!=\"\", map(lambda x: self.word_of(x) if x!= padded_with else \"\", sequence)))\n    \n    def save(self, file_location: Path):\n        pickle.dump(self.word_to_id_dict, open(file_location.joinpath(\"word-to-id-dict.pkl\"), 'wb'))\n        pickle.dump(self.id_to_word_dict, open(file_location.joinpath(\"id-to-word-dict.pkl\"), 'wb'))\n\n    def load(self, file_location: Path):\n        self.word_to_id_dict = pickle.load(open(file_location.joinpath(\"word-to-id-dict.pkl\"), 'rb'))\n        self.id_to_word_dict= pickle.load(open(file_location.joinpath(\"id-to-word-dict.pkl\"), 'rb'))\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"default_vocab_handler = VocabHandler(vocabulary)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logger.info(f\"Vocab Size: {default_vocab_handler.vocab_size}\")\nlogger.info(f\"Id of young is {default_vocab_handler.id_of('young')}\")\nlogger.info(f\"The word corresponding to id 12414 {default_vocab_handler.word_of(12414)}\") \n\nlogger.info(f\"Id of {default_vocab_handler.stop_word} is {default_vocab_handler.id_of(default_vocab_handler.stop_word)}\")\n\n# saving the vocab handler for later use\nVOCAB_HANDLER_SAVE_PATH = WORKING_DIR.joinpath(\"vocab-handler\")\nVOCAB_HANDLER_SAVE_PATH.mkdir(exist_ok=True)\ndefault_vocab_handler.save(VOCAB_HANDLER_SAVE_PATH)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"default_vocab_handler.load(VOCAB_HANDLER_SAVE_PATH)\n\nlogger.info(f\"Vocab Size: {default_vocab_handler.vocab_size}\")\nlogger.info(f\"Id of young is {default_vocab_handler.id_of('young')}\")\nlogger.info(f\"The word corresponding to id 12414 {default_vocab_handler.word_of(12414)}\") \n\nlogger.info(f\"Id of {default_vocab_handler.stop_word} is {default_vocab_handler.id_of(default_vocab_handler.stop_word)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# getting the maximum length of the words in a caption\n# this is important for padding the input as to provide equal length text input\nmaximum_length = max(captions_data[\"caption\"].apply(lambda caption: len(caption.split())))\nlogger.info(f\"The maximum number of words is {maximum_length}\")\n\nabsolute_max_length = maximum_length + 2 # including start and stop words\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Two young guys with shaggy hair look at their\nsample_text = \"<start> Two young guys with shaggy hair look at their <stop>\".lower()\noutput_sequence = default_vocab_handler.text_to_sequence(sample_text, absolute_max_length, padding = True)\noutput_text = default_vocab_handler.sequence_to_text(output_sequence, padded=True)\nlogger.info(f\"Input Text: {sample_text}\")\nlogger.info(f\"Output Sequence: {output_sequence}.\")\nlogger.info(f\"Output Text: {output_text}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Pre-infer images","metadata":{}},{"cell_type":"code","source":"def get_base_model():\n    base_model = VGG19(\n        include_top=True,\n        weights=\"imagenet\",\n        input_tensor=None,\n        input_shape=IMAGE_INPUT_SHAPE,\n        pooling=\"max\",\n    )\n    base_model = Model(inputs=base_model.inputs, outputs=base_model.layers[-2].output)\n    base_model.training = False\n\n    return base_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from contextlib import nullcontext\n\nif USE_MULTIPLE_GPUS:\n    logging.info(\"Using multiple GPUs with mirrored strategy.\")\n    strategy = tf.distribute.MirroredStrategy()\n    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n\nwith strategy.scope() if USE_MULTIPLE_GPUS else nullcontext():\n    base_model = get_base_model()\n    \n    FEATURE_SHAPE = base_model.output.shape\n    logger.info(f\"Feature shape: {FEATURE_SHAPE}\")\n    base_model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_image_features_and_save(images, batch_size=BATCH_SIZE, save_to_file=False) -> dict | None:\n    \"\"\"\n    Returns features if kept in RAM None otherwise\n    \"\"\"\n    \n    features = {}\n    \n    with strategy.scope() if USE_MULTIPLE_GPUS else nullcontext():\n        for i in tqdm(range(0, len(images), batch_size)):\n            img_arr = map(lambda x: img_to_array(load_img(images_dir.joinpath(x), target_size=IMAGE_INPUT_SHAPE)), images[i: i+batch_size])\n            predictions = base_model.predict(preprocess_input(np.array(list(img_arr))), verbose=0)\n\n            for image_name, prediction in zip(images[i: i+batch_size], predictions):\n                features[image_name] = prediction\n\n    if save_to_file:\n        output_file = features_dir.joinpath('features.pkl')\n        pickle.dump(features, open(output_file, 'wb'))\n\n    return features\n\nimages = captions_data[\"image\"].unique().tolist()\n\nif TAKE_FEATURES_FROM_INPUT:\n    with open(features_dir.joinpath('features.pkl'), 'rb') as f:\n        features = pickle.load(f)\nelse:\n    features = extract_image_features_and_save(images, save_to_file=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Generator","metadata":{}},{"cell_type":"code","source":"def data_generator(training_ids: list[str], vocab_handler: VocabHandler, max_length: int, batch_size: int):\n    \"\"\"\n    Generate infinite stream of batches\n    \n    \"\"\"\n    \n    vocab_size = vocab_handler.vocab_size\n    \n    while True:\n        img_features_input, text_inputs, text_outputs = list(), list(), list()\n        \n        random.shuffle(training_ids)\n        sample = 0\n        for img_id in training_ids:\n            feature = features[img_id]\n        \n            # get the captions corresponding to the image_id\n            captions = captions_data.loc[captions_data[\"image\"]==img_id]\n    \n            for caption in captions[\"caption\"].tolist():\n                words = caption.split()\n                words.insert(0, vocab_handler.start_word)\n                words.append(vocab_handler.stop_word)\n                n_words = len(words)\n    \n                for i in range(1, n_words):\n                    img_features_input.append(feature[0])\n                    text_inputs.append(vocab_handler.text_to_sequence(\" \".join(words[:i]), max_length, True))\n                    text_outputs.append(to_categorical([vocab_handler.id_of(words[i])], num_classes=vocab_size)[0])\n        \n                    sample += 1\n            \n                    if sample == batch_size:\n                        sample = 0\n        \n                        img_features_input, text_inputs, text_outputs = np.array(img_features_input), np.array(text_inputs), np.array(text_outputs)\n        \n                        \n                        yield (img_features_input, text_inputs), text_outputs\n            \n                        img_features_input, text_inputs, text_outputs = list(), list(), list()\n                        \n        # Yield any remaining samples that didn't make a full batch\n        if img_features_input:\n            img_batch = np.array(img_features_input)\n            text_batch = np.array(text_inputs)\n            output_batch = np.array(text_outputs)\n            \n            yield (img_batch, text_batch), output_batch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_samples = 2\n_test_images = [\"1000092795.jpg\", \"10002456.jpg\", \"1000268201.jpg\", \"1000344755.jpg\", \"1000366164.jpg\", \"1000523639.jpg\", \"1000919630.jpg\", \"10010052.jpg\", \"1001465944.jpg\", \"1001545525.jpg\", \"1001573224.jpg\", \"1001633352.jpg\", \"1001773457.jpg\", \"1001896054.jpg\", \"100197432.jpg\", \"100207720.jpg\", \"1002674143.jpg\", \"1003163366.jpg\", \"1003420127.jpg\"]\nrandom.shuffle(_test_images)\n_data_generator = data_generator(_test_images,\n                  default_vocab_handler, absolute_max_length, 2)\n\nfor _ in range(n_samples):\n    (img_features, txt_input), text_outputs = _data_generator.__next__()\n\n    for img_feature, txt_i, txt_o in zip(img_features, txt_input, text_outputs):        \n        print(f\"Input: {default_vocab_handler.sequence_to_text(txt_i, True)}\")\n        print(f\"To Predict: {default_vocab_handler.word_of(np.argmax(txt_o))}\")\n        plt.imshow(img_feature.reshape(64, 64, 1))\n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model definition","metadata":{}},{"cell_type":"code","source":"def get_image_captioning_model():\n    \n    feature_input_layer = Input(shape=(FEATURE_SHAPE[1],))\n\n    # some trainable layers before merging\n    dropout_1 = Dropout(0.4)(feature_input_layer)\n    image_feature_output = Dense(256, activation='relu')(dropout_1)\n    \n    # text feature extraction\n    text_input_layer = Input(shape=(absolute_max_length, ))\n    embed = Embedding(default_vocab_handler.vocab_size, 256, mask_zero=True)(text_input_layer)\n    dropout_2 = Dropout(0.4)(embed)\n    text_feature_output = LSTM(256)(dropout_2)\n    \n    # decoding\n    combine = add([image_feature_output, text_feature_output])\n    dense_decoder = Dense(256, activation='relu')(combine)\n    outputs = Dense(default_vocab_handler.vocab_size, activation='softmax')(dense_decoder)\n    \n    model = Model(inputs=[feature_input_layer, text_input_layer], outputs=outputs)\n\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Open a strategy scope if needed.\nwith strategy.scope() if USE_MULTIPLE_GPUS else nullcontext():\n\n    if LOAD_PRETRAINED:\n        logging.info(\"Loading the pretrained model.\")\n        model = tf.keras.models.load_model('/kaggle/input/1024_batch_size_35_epoch/keras/default/1/models.keras')\n    else:\n        model = get_image_captioning_model()\n    \n        # compiling\n        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nplot_model(model, to_file=\"model.png\", show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"markdown","source":"## Setting Up Tensorboard","metadata":{}},{"cell_type":"code","source":"logdir = \"/kaggle/working/logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# doesn't work on kaggle\n\n# # Load the TensorBoard notebook extension.\n# %load_ext tensorboard\n\n# %tensorboard --logdir /kaggle/working/logs/scalars","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setting Up Wandb","metadata":{}},{"cell_type":"code","source":"import wandb\nfrom wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint, WandbCallback\n\nwandb.login(key=\"bed61d44fc0bb5ea4949f930c43ce0f44dcc764a\")\n\n# Initialize a new W&B run\nrun = wandb.init(config={\"bs\": 12}, project=\"Image Caption Generator-Multi-GPU Pre-Infer Images\")\n\nmetric_logger_callback = WandbMetricsLogger(log_freq=\"batch\")\nmodel_save_callback = WandbModelCheckpoint(filepath=\"models.keras\", save_freq=\"epoch\")\nwandb.save(str(VOCAB_HANDLER_SAVE_PATH)+\"/*\", base_path=str(WORKING_DIR))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# reuse old model\nif CONTINUE_FROM_WANDB_RUN:\n  entity = \"kunepal\"\n  project = \"Image Caption Generator-Multi-GPU Pre-Infer Images\"\n  alias = \"latest\"  # semantic nickname or identifier for the model version\n  model_artifact_name = \"run_cq1wzywj_model\"\n\n  # Access and download model. Returns path to downloaded artifact\n\n  downloaded_model_path = run.use_model(name=f\"{entity}/{project}/{model_artifact_name}:{alias}\")\n  run_path = f\"/{entity}/{project}/{model_artifact_name[4:-6]}\"\n  id_to_word_dict_save_path = wandb.restore(\"vocab-handler/id-to-word-dict.pkl\", run_path=run_path)\n  word_to_id_dict_save_path = wandb.restore(\"vocab-handler/word-to-id-dict.pkl\", run_path=run_path)\n\n  loading_path = Path(id_to_word_dict_save_path.name).parent\n  logger.info(f\"Loading from the path {loading_path}.\")\n    \n  default_vocab_handler.load(loading_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Callback to clear memory and restart keras backend at the end of each epoch\nhttps://stackoverflow.com/questions/53683164/keras-occupies-an-indefinitely-increasing-amount-of-memory-for-each-epoch","metadata":{}},{"cell_type":"code","source":"import gc\nfrom tensorflow.keras import backend as k\nfrom tensorflow.keras.callbacks import Callback\n\nclass ClearMemory(Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        gc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping_callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"all_img_ids = captions_data[\"image\"].unique().tolist()\n\nlogger.info(f\"Total samples: {len(all_img_ids)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_total_samples(data):\n    # Calculate the length of captions\n    length_of_caption_df = captions_data.copy()\n    length_of_caption_df[\"length_of_caption\"] = captions_data[\"caption\"].apply(lambda x: len(str(x).split()))\n    \n    # Filter the dataframe to only include images in the given data\n    filtered_df = length_of_caption_df[length_of_caption_df[\"image\"].isin(data)]\n    \n    # Sum the lengths directly\n    total_samples = filtered_df[\"length_of_caption\"].sum()\n    \n    return total_samples\n\ncalculate_total_samples([\"1000092795.jpg\", \"1000366164.jpg\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train, test_and_validation = train_test_split(all_img_ids, test_size=((TRAIN_TEST_VAL_SPLIT[1] + TRAIN_TEST_VAL_SPLIT[2])/ sum(TRAIN_TEST_VAL_SPLIT)))\ntest, validation = train_test_split(test_and_validation, test_size=TRAIN_TEST_VAL_SPLIT[2]/(TRAIN_TEST_VAL_SPLIT[1] + TRAIN_TEST_VAL_SPLIT[2]))\n\ntotal_training_samples = calculate_total_samples(train)\ntotal_validation_samples = calculate_total_samples(validation)\n\nsteps = total_training_samples // BATCH_SIZE\nval_steps = total_validation_samples // BATCH_SIZE\n\nlogger.info(f\"Total batches in an epoch: {steps}\")\nlogger.info(f\"Total batches in validation set: {val_steps}\")\n\ngenerator = data_generator(train, default_vocab_handler, absolute_max_length, BATCH_SIZE)\nval_generator = data_generator(validation, default_vocab_handler, absolute_max_length, BATCH_SIZE)\ntest_generator = data_generator(test, default_vocab_handler, absolute_max_length, BATCH_SIZE)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope() if USE_MULTIPLE_GPUS else nullcontext():\n    if CONTINUE_FROM_WANDB_RUN:\n        model = tf.keras.models.load_model(downloaded_model_path)\n        logger.info(f\"Continuing from the previous training sample from wandb\")\n    history = model.fit(\n        generator, \n        epochs=N_EPOCHS, \n        steps_per_epoch=steps, \n        verbose=1, validation_data=val_generator,\n        callbacks=[tensorboard_callback, metric_logger_callback, model_save_callback, ClearMemory(), early_stopping_callback], \n        validation_freq=1, \n        validation_steps=val_steps)\n    if EVALUATE_AFTER_TRAIN:\n        model.evaluate(test_generator)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Saving and loading up the saved model to remove dependencies of multiple GPUS","metadata":{}},{"cell_type":"code","source":"# this is done because model trained using multiple-GPUS require more than one \n# sample in a batch to distribute it equally\n# since this is a bit complicated to run inference on multiple image,text pair\n# so, using this hack for now\nmodel.save('temp_model.keras')\n\nmodel = tf.keras.models.load_model('temp_model.keras')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing and visualization on few images","metadata":{}},{"cell_type":"code","source":"def test_on_image_having_feature(image_id: str):\n\n    _feature = features[image_id]\n    \n    text_input = \"<start>\"\n    whole_text_output = \"\"\n    for i in range(absolute_max_length):\n        sequence_input = default_vocab_handler.text_to_sequence(text_input, absolute_max_length, True)\n\n        model_input = [_feature.reshape((1, FEATURE_SHAPE[1])), sequence_input.reshape((1,absolute_max_length))]\n        predictions = model.predict(model_input, verbose=0)\n        sequence_output = np.argmax(predictions[0])\n        \n        text_output = default_vocab_handler.word_of(sequence_output)\n        \n        if text_output == default_vocab_handler.stop_word:\n            break\n        whole_text_output += \" \" + text_output\n        text_input += \" \" + text_output\n    \n    return whole_text_output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_images_to_test = 2\n\ntest_images = random.sample(test, n_images_to_test)\n    \nfor image_id in test_images:    \n    whole_text_output = test_on_image_having_feature(image_id)\n    img_path = images_dir.joinpath(image_id)\n    \n    print(f\"Generated: {whole_text_output}\")\n    print(f\"Actual: {captions_data.loc[captions_data['image'] == image_id]['caption'].tolist()}\")\n    plt.imshow(load_img(img_path, target_size=IMAGE_INPUT_SHAPE))\n    plt.show()\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_on_new_image(image: Path | np.ndarray):\n    if isinstance(image, Path):\n        image = load_img(image, target_size=IMAGE_INPUT_SHAPE)\n        image = img_to_array(image)\n\n    reshaped_img = image.reshape(1, *IMAGE_INPUT_SHAPE)\n    image_input = preprocess_input(reshaped_img)\n\n    _feature = base_model.predict(image_input)[0]\n    \n    text_input = \"<start> \"\n    whole_text_output = \"\"\n    for i in range(absolute_max_length):\n        sequence_input = default_vocab_handler.text_to_sequence(text_input, absolute_max_length, True)\n\n        model_input = [_feature.reshape((1, FEATURE_SHAPE[1])), sequence_input.reshape((1,absolute_max_length))]\n        \n        sequence_output = np.argmax(model.predict(model_input, verbose=0)[0])\n        \n        text_output = default_vocab_handler.word_of(sequence_output)\n        \n        if text_output == default_vocab_handler.stop_word:\n            break\n        whole_text_output += \" \" + text_output\n        text_input += \" \" + text_output\n    \n    return whole_text_output\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_images_to_test = 2\n\ntest_images = random.sample(test, n_images_to_test)\n    \nfor image_id in test_images:    \n    img_path = images_dir.joinpath(image_id)\n    whole_text_output = test_on_new_image(img_path)\n    \n    print(f\"Generated: {whole_text_output}\")\n    print(f\"Actual: {captions_data.loc[captions_data['image'] == image_id]['caption'].tolist()}\")\n    plt.imshow(load_img(img_path, target_size=IMAGE_INPUT_SHAPE))\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing BLEU Score","metadata":{}},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\nfrom tqdm import tqdm\n\nactual = []\npredicted = []\nfor image_id in tqdm(test[:3]):\n    whole_text_output = test_on_image_having_feature(image_id)\n\n    _actual = list(map(lambda x: x.split(), captions_data.loc[captions_data['image'] == image_id]['caption'].tolist()))\n    _predicted = whole_text_output.split()\n    \n    actual.append(_actual)\n    predicted.append(_predicted)\n\n    logger.info(f\"\\n\\nActual: {_actual} \\n --- \\nPredicted: {_predicted}\")\n    \nscore = corpus_bleu(actual, predicted)\n\nlogger.info(f\"Score: {score}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Parallelized","metadata":{}},{"cell_type":"code","source":"with strategy.scope() if USE_MULTIPLE_GPUS else nullcontext():\n    model = tf.keras.models.load_model('temp_model.keras')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_parallelized(images, batch_size):\n\n    predicted: list[str] = []\n    \n    for i in tqdm(range(0, len(images), batch_size)):\n        images_batch = images[i: i + batch_size]\n        num_images_in_batch = len(images_batch)\n        \n        text_inputs = [\"<start>\" for _ in range(num_images_in_batch)]\n        text_outputs = [\"\" for _ in range(num_images_in_batch)]\n\n        sequence_inputs = np.zeros(shape=(num_images_in_batch, absolute_max_length))\n        feature_inputs = np.zeros(shape=(num_images_in_batch, FEATURE_SHAPE[1]))\n\n        for j in range(absolute_max_length):\n            for k, image in enumerate(images_batch):\n                sequence_inputs[k] = default_vocab_handler.text_to_sequence(text_inputs[k], absolute_max_length, True)\n                feature_inputs[k] = features[image]\n\n            model_inputs = [feature_inputs, sequence_inputs]\n            \n            sequence_outputs = list(map(lambda x: np.argmax(x), model.predict(model_inputs, verbose=0)))\n            \n            text_output = list(map(lambda x: default_vocab_handler.word_of(x), sequence_outputs))\n\n            for l in range(num_images_in_batch):\n                text_outputs[l] += \" \" + text_output[l]\n                text_inputs[l] += \" \" + text_output[l]\n\n        predicted.extend(text_outputs)\n\n    return predicted\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom tqdm import tqdm\n\nbatch_size = 32\nwith strategy.scope() if USE_MULTIPLE_GPUS else nullcontext():\n    predicted = test_parallelized(test, batch_size)\n\nactual = []\nfor i, image_id in tqdm(enumerate(test)):\n    # since all the inference is run up to maxlength, the generated text\n    # might contain many <stop> i.e. stop words\n    # so discarding after encountering the first stop word\n    first_index_of_end_sequence = predicted[i].find(default_vocab_handler.stop_word)\n    predicted[i] = predicted[i][0: first_index_of_end_sequence].split()\n    \n    _actual = list(map(lambda x: x.split(), captions_data.loc[captions_data['image'] == image_id]['caption'].tolist()))\n    actual.append(_actual)\n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"score = corpus_bleu(actual, predicted)\nlogger.info(f\"Bleu score: {score}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_generator.__next__()[]","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}