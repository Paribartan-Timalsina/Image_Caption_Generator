{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T14:59:35.717202Z",
     "iopub.status.busy": "2024-12-04T14:59:35.716685Z",
     "iopub.status.idle": "2024-12-04T14:59:36.898756Z",
     "shell.execute_reply": "2024-12-04T14:59:36.897340Z",
     "shell.execute_reply.started": "2024-12-04T14:59:35.717153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls /kaggle/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T13:01:20.673357Z",
     "iopub.status.busy": "2024-12-04T13:01:20.672463Z",
     "iopub.status.idle": "2024-12-04T13:01:33.839498Z",
     "shell.execute_reply": "2024-12-04T13:01:33.838138Z",
     "shell.execute_reply.started": "2024-12-04T13:01:20.673295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow keras tensorboard scikit-learn ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T15:25:51.219119Z",
     "iopub.status.busy": "2024-12-04T15:25:51.218750Z",
     "iopub.status.idle": "2024-12-04T15:26:02.866217Z",
     "shell.execute_reply": "2024-12-04T15:26:02.865491Z",
     "shell.execute_reply.started": "2024-12-04T15:25:51.219085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import random\n",
    "from PIL import Image \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, LSTM, add, Flatten\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# notebook specific\n",
    "from IPython.display import display\n",
    "# from tqdm.notebook import tqdm # for some reason this doesn't work\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "TRAIN_CNN: bool = False\n",
    "NUM_OUTPUT_CAPTIONS: int = 1\n",
    "IMAGE_INPUT_SHAPE: tuple[int, int, int] = (224, 224, 3) # (height, width, channel)\n",
    "\n",
    "DATASET_NAME: str = \"flickr30k\"\n",
    "FEATURES_NAME: str = \"flickr-31k-features-all\"\n",
    "\n",
    "FILTER_NON_ALPHA_NUMERIC_STRINGS = True\n",
    "\n",
    "EVALUATE_AFTER_TRAIN: bool = False\n",
    "LOAD_PRETRAINED: bool = False # make it possible to continue training from a saved model\n",
    "TAKE_FEATURES_FROM_INPUT: bool = True # load features from already saved file as infering this takes a long time\n",
    "CONTINUE_FROM_WANDB_RUN: bool = False # if needed to load pretrained model from wandb\n",
    "USE_MULTIPLE_GPUS: bool = True \n",
    "\n",
    "# Hyperparameters\n",
    "N_EPOCHS: int = 35\n",
    "BATCH_SIZE: int = 1024\n",
    "DEBUG: bool = False\n",
    "TRAIN_TEST_VAL_SPLIT = (70, 20, 10)\n",
    "    \n",
    "# paths\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "INPUT_DIR = Path(\"/kaggle/input/\")\n",
    "DATASET_INPUT_DIR = INPUT_DIR.joinpath(DATASET_NAME)\n",
    "TEMP_DIR = Path(\"/tmp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# setting up the logger\n",
    "logging.basicConfig(level=logging.DEBUG if DEBUG else logging.INFO, force=True) # a workaround\n",
    "logger = logging.getLogger(\"-^-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_good = True\n",
    "\n",
    "if not os.path.exists(WORKING_DIR):\n",
    "    logger.error(f\"The directory {WORKING_DIR} doesn't exist.\")\n",
    "    all_good = False\n",
    "\n",
    "if not os.path.exists(DATASET_INPUT_DIR):\n",
    "    logger.error(f\"The directory {DATASET_INPUT_DIR} doesn't exist.\")\n",
    "    all_good = False\n",
    "\n",
    "if not os.path.exists(TEMP_DIR):\n",
    "    logger.error(f\"The directory {TEMP_DIR} doesn't exist.\")\n",
    "    all_good = False\n",
    "\n",
    "if all_good:\n",
    "    logger.info(f\"All the directories are valid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images_dir = DATASET_INPUT_DIR.joinpath(\"Images\")\n",
    "captions_file = DATASET_INPUT_DIR.joinpath(\"captions.txt\")\n",
    "\n",
    "if TAKE_FEATURES_FROM_INPUT:\n",
    "    features_dir = INPUT_DIR.joinpath(FEATURES_NAME)\n",
    "else:\n",
    "    features_dir = WORKING_DIR.joinpath(\"features\")\n",
    "    \n",
    "    features_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "captions_data = pd.read_csv(captions_file)\n",
    "captions_data.astype(str)\n",
    "captions_data.dropna(inplace=True)\n",
    "logger.info(captions_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# filtering the columns having too few or too many words\n",
    "# as having too few and too many can improperly skew whole training process\n",
    "# having too many causes the whole network to be trained mostly on padding rather than the actual data\n",
    "\n",
    "upper_limit = 30\n",
    "lower_limit = 6\n",
    "\n",
    "captions_data = captions_data.drop(captions_data.loc[captions_data[\"caption\"].apply(lambda x: len(str(x).split()) > upper_limit or len(str(x).split()) < lower_limit)].index)\n",
    "\n",
    "len(captions_data[\"image\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unfiltered_vocabulary = list(set(\" \".join(captions_data[\"caption\"].to_list()).lower().split()))\n",
    "\n",
    "removed_items = []\n",
    "\n",
    "if FILTER_NON_ALPHA_NUMERIC_STRINGS:\n",
    "    vocabulary = list(filter(lambda x: len(x) >= 3 or x.isalpha() , unfiltered_vocabulary))\n",
    "    removed_items += list(filter(lambda x: len(x) < 3 and not x.isalpha() , unfiltered_vocabulary))\n",
    "else:\n",
    "    vocabulary = unfiltered_vocabulary\n",
    "\n",
    "filtered_captions_data = captions_data.copy()\n",
    "filtered_captions_data[\"caption\"] = filtered_captions_data[\"caption\"].apply(\n",
    "    lambda x: \" \".join(list(filter(\n",
    "        lambda y: y not in removed_items, x.lower().split()))\n",
    "                      )\n",
    ")\n",
    "\n",
    "vocabulary_from_filtered_captions_data = list(set(\" \".join(filtered_captions_data[\"caption\"].to_list()).lower().split()))\n",
    "\n",
    "for x in vocabulary_from_filtered_captions_data:\n",
    "    if x not in vocabulary:\n",
    "        logger.error(f\"Word: '{x}' not in vocabulary\")\n",
    "        raise Exception(\"Found a word that is not in the vocabulary\")\n",
    "\n",
    "captions_data = filtered_captions_data\n",
    "\n",
    "logger.info(f\"Removed items: {removed_items}\")\n",
    "\n",
    "logger.info(f\"Total unique words: {len(vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VocabHandler():\n",
    "    \"\"\"\n",
    "    Handles vocabulary. Indices start from 1 since 0 is reserved for padding.\n",
    "    \"\"\"\n",
    "    word_to_id_dict: dict[str, int] = {}\n",
    "    id_to_word_dict: dict[int, str] = {}\n",
    "    vocab_size = 0\n",
    "        \n",
    "    def __init__(self, vocabulary: list[str], start_word: str = \"<start>\", stop_word: str = \"<stop>\", count_padding_as_separate_word: bool = True, padding: str = \"<padding>\"):\n",
    "        \"\"\"\n",
    "        ID 0 is used for padding\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        vocabulary: The list of words\n",
    "        start_word: special word for denoting the start of generation\n",
    "        stop_word: special word for denoting the stop of generation\n",
    "        count_padding_as_separate_word: if false, there won't be a entry for ID 0\n",
    "        padding: the special word place where id=0\n",
    "        \"\"\"\n",
    "        \n",
    "        self.start_word: str = start_word\n",
    "        self.stop_word: str = stop_word\n",
    "        \n",
    "        if count_padding_as_separate_word:\n",
    "            self.padding = padding\n",
    "            self.word_to_id_dict[self.padding] = 0\n",
    "            self.id_to_word_dict[0] = self.padding\n",
    "        \n",
    "        # adding start word in the vocabulary\n",
    "        self.word_to_id_dict[self.start_word] = 1\n",
    "        self.id_to_word_dict[1] = self.start_word\n",
    "        \n",
    "        last_index = 0\n",
    "        for idx, word in enumerate(vocabulary):\n",
    "            self.word_to_id_dict[word] = idx + 2\n",
    "            self.id_to_word_dict[idx + 2] = word \n",
    "            last_index = idx + 2\n",
    "            \n",
    "        # adding start word in the vocabulary\n",
    "        self.word_to_id_dict[self.stop_word] = last_index + 1\n",
    "        self.id_to_word_dict[last_index + 1] = self.stop_word\n",
    "        \n",
    "        assert len(self.word_to_id_dict) == len(self.id_to_word_dict)\n",
    "        \n",
    "        self.vocab_size = len(self.word_to_id_dict)\n",
    "\n",
    "    def id_of(self, word: str) -> int | None:\n",
    "        return self.word_to_id_dict[word]\n",
    "\n",
    "    def word_of(self, idx: int) -> str | None:\n",
    "        return self.id_to_word_dict[idx]\n",
    "    \n",
    "    def text_to_sequence(self, text: str, max_length: int = 0, padding: bool = False, pad_with: int = 0) -> np.ndarray:\n",
    "        \n",
    "        words = text.split()\n",
    "        \n",
    "        if not padding or max_length < 1:\n",
    "            if max_length < 1:\n",
    "                logger.error(f\"The provided maximum length {max_length} is invalid.\")\n",
    "            return np.array(list(map(lambda x: self.id_of(x), words)))\n",
    "        \n",
    "        len_words = len(words)\n",
    "        \n",
    "        padded_sequence = np.full(max_length, pad_with)\n",
    "        padded_sequence[:len_words] = np.array(list(map(lambda x: self.id_of(x), words)))\n",
    "        \n",
    "        \n",
    "        return padded_sequence\n",
    "    \n",
    "    def sequence_to_text(self, sequence: np.ndarray, padded: bool = False, padded_with: int = 0):\n",
    "        if not padded:\n",
    "            return \" \".join(map(lambda x: self.word_of(x), sequence))\n",
    "        \n",
    "        return \" \".join(filter(lambda y: y!=\"\", map(lambda x: self.word_of(x) if x!= padded_with else \"\", sequence)))\n",
    "    \n",
    "    def save(self, file_location: Path):\n",
    "        pickle.dump(self.word_to_id_dict, open(file_location.joinpath(\"word-to-id-dict.pkl\"), 'wb'))\n",
    "        pickle.dump(self.id_to_word_dict, open(file_location.joinpath(\"id-to-word-dict.pkl\"), 'wb'))\n",
    "\n",
    "    def load(self, file_location: Path):\n",
    "        self.word_to_id_dict = pickle.load(open(file_location.joinpath(\"word-to-id-dict.pkl\"), 'rb'))\n",
    "        self.id_to_word_dict= pickle.load(open(file_location.joinpath(\"id-to-word-dict.pkl\"), 'rb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "default_vocab_handler = VocabHandler(vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Vocab Size: {default_vocab_handler.vocab_size}\")\n",
    "logger.info(f\"Id of young is {default_vocab_handler.id_of('young')}\")\n",
    "logger.info(f\"The word corresponding to id 12414 {default_vocab_handler.word_of(12414)}\") \n",
    "\n",
    "logger.info(f\"Id of {default_vocab_handler.stop_word} is {default_vocab_handler.id_of(default_vocab_handler.stop_word)}\")\n",
    "\n",
    "# saving the vocab handler for later use\n",
    "VOCAB_HANDLER_SAVE_PATH = WORKING_DIR.joinpath(\"vocab-handler\")\n",
    "VOCAB_HANDLER_SAVE_PATH.mkdir(exist_ok=True)\n",
    "default_vocab_handler.save(VOCAB_HANDLER_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "default_vocab_handler.load(VOCAB_HANDLER_SAVE_PATH)\n",
    "\n",
    "logger.info(f\"Vocab Size: {default_vocab_handler.vocab_size}\")\n",
    "logger.info(f\"Id of young is {default_vocab_handler.id_of('young')}\")\n",
    "logger.info(f\"The word corresponding to id 12414 {default_vocab_handler.word_of(12414)}\") \n",
    "\n",
    "logger.info(f\"Id of {default_vocab_handler.stop_word} is {default_vocab_handler.id_of(default_vocab_handler.stop_word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# getting the maximum length of the words in a caption\n",
    "# this is important for padding the input as to provide equal length text input\n",
    "maximum_length = max(captions_data[\"caption\"].apply(lambda caption: len(caption.split())))\n",
    "logger.info(f\"The maximum number of words is {maximum_length}\")\n",
    "\n",
    "absolute_max_length = maximum_length + 2 # including start and stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Two young guys with shaggy hair look at their\n",
    "sample_text = \"<start> Two young guys with shaggy hair look at their <stop>\".lower()\n",
    "output_sequence = default_vocab_handler.text_to_sequence(sample_text, absolute_max_length, padding = True)\n",
    "output_text = default_vocab_handler.sequence_to_text(output_sequence, padded=True)\n",
    "logger.info(f\"Input Text: {sample_text}\")\n",
    "logger.info(f\"Output Sequence: {output_sequence}.\")\n",
    "logger.info(f\"Output Text: {output_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-infer images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_base_model():\n",
    "    base_model = VGG19(\n",
    "        include_top=True,\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=None,\n",
    "        input_shape=IMAGE_INPUT_SHAPE,\n",
    "        pooling=\"max\",\n",
    "    )\n",
    "    base_model = Model(inputs=base_model.inputs, outputs=base_model.layers[-2].output)\n",
    "    base_model.training = False\n",
    "\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "\n",
    "if USE_MULTIPLE_GPUS:\n",
    "    logging.info(\"Using multiple GPUs with mirrored strategy.\")\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope() if USE_MULTIPLE_GPUS else nullcontext():\n",
    "    base_model = get_base_model()\n",
    "    \n",
    "    FEATURE_SHAPE = base_model.output.shape\n",
    "    logger.info(f\"Feature shape: {FEATURE_SHAPE}\")\n",
    "    base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_image_features_and_save(images, batch_size=BATCH_SIZE, save_to_file=False) -> dict | None:\n",
    "    \"\"\"\n",
    "    Returns features if kept in RAM None otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    with strategy.scope() if USE_MULTIPLE_GPUS else nullcontext():\n",
    "        for i in tqdm(range(0, len(images), batch_size)):\n",
    "            img_arr = map(lambda x: img_to_array(load_img(images_dir.joinpath(x), target_size=IMAGE_INPUT_SHAPE)), images[i: i+batch_size])\n",
    "            predictions = base_model.predict(preprocess_input(np.array(list(img_arr))), verbose=0)\n",
    "\n",
    "            for image_name, prediction in zip(images[i: i+batch_size], predictions):\n",
    "                features[image_name] = prediction\n",
    "\n",
    "    if save_to_file:\n",
    "        output_file = features_dir.joinpath('features.pkl')\n",
    "        pickle.dump(features, open(output_file, 'wb'))\n",
    "\n",
    "    return features\n",
    "\n",
    "images = captions_data[\"image\"].unique().tolist()\n",
    "\n",
    "if TAKE_FEATURES_FROM_INPUT:\n",
    "    with open(features_dir.joinpath('features.pkl'), 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "else:\n",
    "    features = extract_image_features_and_save(images, save_to_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def data_generator(training_ids: list[str], vocab_handler: VocabHandler, max_length: int, batch_size: int):\n",
    "    \"\"\"\n",
    "    Generate infinite stream of batches\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_size = vocab_handler.vocab_size\n",
    "    \n",
    "    while True:\n",
    "        img_features_input, text_inputs, text_outputs = list(), list(), list()\n",
    "        \n",
    "        random.shuffle(training_ids)\n",
    "        sample = 0\n",
    "        for img_id in training_ids:\n",
    "            feature = features[img_id]\n",
    "        \n",
    "            # get the captions corresponding to the image_id\n",
    "            captions = captions_data.loc[captions_data[\"image\"]==img_id]\n",
    "    \n",
    "            for caption in captions[\"caption\"].tolist():\n",
    "                words = caption.split()\n",
    "                words.insert(0, vocab_handler.start_word)\n",
    "                words.append(vocab_handler.stop_word)\n",
    "                n_words = len(words)\n",
    "    \n",
    "                for i in range(1, n_words):\n",
    "                    img_features_input.append(feature[0])\n",
    "                    text_inputs.append(vocab_handler.text_to_sequence(\" \".join(words[:i]), max_length, True))\n",
    "                    text_outputs.append(to_categorical([vocab_handler.id_of(words[i])], num_classes=vocab_size)[0])\n",
    "        \n",
    "                    sample += 1\n",
    "            \n",
    "                    if sample == batch_size:\n",
    "                        sample = 0\n",
    "        \n",
    "                        img_features_input, text_inputs, text_outputs = np.array(img_features_input), np.array(text_inputs), np.array(text_outputs)\n",
    "        \n",
    "                        \n",
    "                        yield (img_features_input, text_inputs), text_outputs\n",
    "            \n",
    "                        img_features_input, text_inputs, text_outputs = list(), list(), list()\n",
    "                        \n",
    "        # Yield any remaining samples that didn't make a full batch\n",
    "        if img_features_input:\n",
    "            img_batch = np.array(img_features_input)\n",
    "            text_batch = np.array(text_inputs)\n",
    "            output_batch = np.array(text_outputs)\n",
    "            \n",
    "            yield (img_batch, text_batch), output_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2\n",
    "_test_images = [\"1000092795.jpg\", \"10002456.jpg\", \"1000268201.jpg\", \"1000344755.jpg\", \"1000366164.jpg\", \"1000523639.jpg\", \"1000919630.jpg\", \"10010052.jpg\", \"1001465944.jpg\", \"1001545525.jpg\", \"1001573224.jpg\", \"1001633352.jpg\", \"1001773457.jpg\", \"1001896054.jpg\", \"100197432.jpg\", \"100207720.jpg\", \"1002674143.jpg\", \"1003163366.jpg\", \"1003420127.jpg\"]\n",
    "random.shuffle(_test_images)\n",
    "_data_generator = data_generator(_test_images,\n",
    "                  default_vocab_handler, absolute_max_length, 2)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    (img_features, txt_input), text_outputs = _data_generator.__next__()\n",
    "\n",
    "    for img_feature, txt_i, txt_o in zip(img_features, txt_input, text_outputs):        \n",
    "        print(f\"Input: {default_vocab_handler.sequence_to_text(txt_i, True)}\")\n",
    "        print(f\"To Predict: {default_vocab_handler.word_of(np.argmax(txt_o))}\")\n",
    "        plt.imshow(img_feature.reshape(64, 64, 1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_image_captioning_model():\n",
    "    \n",
    "    feature_input_layer = Input(shape=(FEATURE_SHAPE[1],))\n",
    "\n",
    "    # some trainable layers before merging\n",
    "    dropout_1 = Dropout(0.4)(feature_input_layer)\n",
    "    image_feature_output = Dense(256, activation='relu')(dropout_1)\n",
    "    \n",
    "    # text feature extraction\n",
    "    text_input_layer = Input(shape=(absolute_max_length, ))\n",
    "    embed = Embedding(default_vocab_handler.vocab_size, 256, mask_zero=True)(text_input_layer)\n",
    "    dropout_2 = Dropout(0.4)(embed)\n",
    "    text_feature_output = LSTM(256)(dropout_2)\n",
    "    \n",
    "    # decoding\n",
    "    combine = add([image_feature_output, text_feature_output])\n",
    "    dense_decoder = Dense(256, activation='relu')(combine)\n",
    "    outputs = Dense(default_vocab_handler.vocab_size, activation='softmax')(dense_decoder)\n",
    "    \n",
    "    model = Model(inputs=[feature_input_layer, text_input_layer], outputs=outputs)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Open a strategy scope if needed.\n",
    "with strategy.scope() if USE_MULTIPLE_GPUS else nullcontext():\n",
    "\n",
    "    if LOAD_PRETRAINED:\n",
    "        logging.info(\"Loading the pretrained model.\")\n",
    "        model = tf.keras.models.load_model('/kaggle/input/1024_batch_size_35_epoch/keras/default/1/models.keras')\n",
    "    else:\n",
    "        model = get_image_captioning_model()\n",
    "    \n",
    "        # compiling\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "plot_model(model, to_file=\"model.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "logdir = \"/kaggle/working/logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# doesn't work on kaggle\n",
    "\n",
    "# # Load the TensorBoard notebook extension.\n",
    "# %load_ext tensorboard\n",
    "\n",
    "# %tensorboard --logdir /kaggle/working/logs/scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint, WandbCallback\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "# Initialize a new W&B run\n",
    "run = wandb.init(config={\"bs\": 12}, project=\"Image Caption Generator-Multi-GPU Pre-Infer Images\")\n",
    "\n",
    "metric_logger_callback = WandbMetricsLogger(log_freq=\"batch\")\n",
    "model_save_callback = WandbModelCheckpoint(filepath=\"models.keras\", save_freq=\"epoch\")\n",
    "wandb.save(str(VOCAB_HANDLER_SAVE_PATH)+\"/*\", base_path=str(WORKING_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# reuse old model\n",
    "if CONTINUE_FROM_WANDB_RUN:\n",
    "  entity = \"kunepal\"\n",
    "  project = \"Image Caption Generator-Multi-GPU Pre-Infer Images\"\n",
    "  alias = \"latest\"  # semantic nickname or identifier for the model version\n",
    "  model_artifact_name = \"run_cq1wzywj_model\"\n",
    "\n",
    "  # Access and download model. Returns path to downloaded artifact\n",
    "\n",
    "  downloaded_model_path = run.use_model(name=f\"{entity}/{project}/{model_artifact_name}:{alias}\")\n",
    "  run_path = f\"/{entity}/{project}/{model_artifact_name[4:-6]}\"\n",
    "  id_to_word_dict_save_path = wandb.restore(\"vocab-handler/id-to-word-dict.pkl\", run_path=run_path)\n",
    "  word_to_id_dict_save_path = wandb.restore(\"vocab-handler/word-to-id-dict.pkl\", run_path=run_path)\n",
    "\n",
    "  loading_path = Path(id_to_word_dict_save_path.name).parent\n",
    "  logger.info(f\"Loading from the path {loading_path}.\")\n",
    "    \n",
    "  default_vocab_handler.load(loading_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback to clear memory and restart keras backend at the end of each epoch\n",
    "https://stackoverflow.com/questions/53683164/keras-occupies-an-indefinitely-increasing-amount-of-memory-for-each-epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class ClearMemory(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "early_stopping_callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_img_ids = captions_data[\"image\"].unique().tolist()\n",
    "\n",
    "logger.info(f\"Total samples: {len(all_img_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_total_samples(data):\n",
    "    # Calculate the length of captions\n",
    "    length_of_caption_df = captions_data.copy()\n",
    "    length_of_caption_df[\"length_of_caption\"] = captions_data[\"caption\"].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Filter the dataframe to only include images in the given data\n",
    "    filtered_df = length_of_caption_df[length_of_caption_df[\"image\"].isin(data)]\n",
    "    \n",
    "    # Sum the lengths directly\n",
    "    total_samples = filtered_df[\"length_of_caption\"].sum()\n",
    "    \n",
    "    return total_samples\n",
    "\n",
    "calculate_total_samples([\"1000092795.jpg\", \"1000366164.jpg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train, test_and_validation = train_test_split(all_img_ids, test_size=((TRAIN_TEST_VAL_SPLIT[1] + TRAIN_TEST_VAL_SPLIT[2])/ sum(TRAIN_TEST_VAL_SPLIT)))\n",
    "test, validation = train_test_split(test_and_validation, test_size=TRAIN_TEST_VAL_SPLIT[2]/(TRAIN_TEST_VAL_SPLIT[1] + TRAIN_TEST_VAL_SPLIT[2]))\n",
    "\n",
    "total_training_samples = calculate_total_samples(train)\n",
    "total_validation_samples = calculate_total_samples(validation)\n",
    "\n",
    "steps = total_training_samples // BATCH_SIZE\n",
    "val_steps = total_validation_samples // BATCH_SIZE\n",
    "\n",
    "logger.info(f\"Total batches in an epoch: {steps}\")\n",
    "logger.info(f\"Total batches in validation set: {val_steps}\")\n",
    "\n",
    "generator = data_generator(train, default_vocab_handler, absolute_max_length, BATCH_SIZE)\n",
    "val_generator = data_generator(validation, default_vocab_handler, absolute_max_length, BATCH_SIZE)\n",
    "test_generator = data_generator(test, default_vocab_handler, absolute_max_length, BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with strategy.scope() if USE_MULTIPLE_GPUS else nullcontext():\n",
    "    if CONTINUE_FROM_WANDB_RUN:\n",
    "        model = tf.keras.models.load_model(downloaded_model_path)\n",
    "        logger.info(f\"Continuing from the previous training sample from wandb\")\n",
    "    history = model.fit(\n",
    "        generator, \n",
    "        epochs=N_EPOCHS, \n",
    "        steps_per_epoch=steps, \n",
    "        verbose=1, validation_data=val_generator,\n",
    "        callbacks=[tensorboard_callback, metric_logger_callback, model_save_callback, ClearMemory(), early_stopping_callback], \n",
    "        validation_freq=1, \n",
    "        validation_steps=val_steps)\n",
    "    if EVALUATE_AFTER_TRAIN:\n",
    "        model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading up the saved model to remove dependencies of multiple GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# this is done because model trained using multiple-GPUS require more than one \n",
    "# sample in a batch to distribute it equally\n",
    "# since this is a bit complicated to run inference on multiple image,text pair\n",
    "# so, using this hack for now\n",
    "model.save('temp_model.keras')\n",
    "\n",
    "model = tf.keras.models.load_model('temp_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and visualization on few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_on_image_having_feature(image_id: str):\n",
    "\n",
    "    _feature = features[image_id]\n",
    "    \n",
    "    text_input = \"<start>\"\n",
    "    whole_text_output = \"\"\n",
    "    for i in range(absolute_max_length):\n",
    "        sequence_input = default_vocab_handler.text_to_sequence(text_input, absolute_max_length, True)\n",
    "\n",
    "        model_input = [_feature.reshape((1, FEATURE_SHAPE[1])), sequence_input.reshape((1,absolute_max_length))]\n",
    "        predictions = model.predict(model_input, verbose=0)\n",
    "        sequence_output = np.argmax(predictions[0])\n",
    "        \n",
    "        text_output = default_vocab_handler.word_of(sequence_output)\n",
    "        \n",
    "        if text_output == default_vocab_handler.stop_word:\n",
    "            break\n",
    "        whole_text_output += \" \" + text_output\n",
    "        text_input += \" \" + text_output\n",
    "    \n",
    "    return whole_text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_images_to_test = 2\n",
    "\n",
    "test_images = random.sample(test, n_images_to_test)\n",
    "    \n",
    "for image_id in test_images:    \n",
    "    whole_text_output = test_on_image_having_feature(image_id)\n",
    "    img_path = images_dir.joinpath(image_id)\n",
    "    \n",
    "    print(f\"Generated: {whole_text_output}\")\n",
    "    print(f\"Actual: {captions_data.loc[captions_data['image'] == image_id]['caption'].tolist()}\")\n",
    "    plt.imshow(load_img(img_path, target_size=IMAGE_INPUT_SHAPE))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_on_new_image(image: Path | np.ndarray):\n",
    "    if isinstance(image, Path):\n",
    "        image = load_img(image, target_size=IMAGE_INPUT_SHAPE)\n",
    "        image = img_to_array(image)\n",
    "\n",
    "    reshaped_img = image.reshape(1, *IMAGE_INPUT_SHAPE)\n",
    "    image_input = preprocess_input(reshaped_img)\n",
    "\n",
    "    _feature = base_model.predict(image_input)[0]\n",
    "    \n",
    "    text_input = \"<start> \"\n",
    "    whole_text_output = \"\"\n",
    "    for i in range(absolute_max_length):\n",
    "        sequence_input = default_vocab_handler.text_to_sequence(text_input, absolute_max_length, True)\n",
    "\n",
    "        model_input = [_feature.reshape((1, FEATURE_SHAPE[1])), sequence_input.reshape((1,absolute_max_length))]\n",
    "        \n",
    "        sequence_output = np.argmax(model.predict(model_input, verbose=0)[0])\n",
    "        \n",
    "        text_output = default_vocab_handler.word_of(sequence_output)\n",
    "        \n",
    "        if text_output == default_vocab_handler.stop_word:\n",
    "            break\n",
    "        whole_text_output += \" \" + text_output\n",
    "        text_input += \" \" + text_output\n",
    "    \n",
    "    return whole_text_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_images_to_test = 2\n",
    "\n",
    "test_images = random.sample(test, n_images_to_test)\n",
    "    \n",
    "for image_id in test_images:    \n",
    "    img_path = images_dir.joinpath(image_id)\n",
    "    whole_text_output = test_on_new_image(img_path)\n",
    "    \n",
    "    print(f\"Generated: {whole_text_output}\")\n",
    "    print(f\"Actual: {captions_data.loc[captions_data['image'] == image_id]['caption'].tolist()}\")\n",
    "    plt.imshow(load_img(img_path, target_size=IMAGE_INPUT_SHAPE))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "actual = []\n",
    "predicted = []\n",
    "for image_id in tqdm(test[:3]):\n",
    "    whole_text_output = test_on_image_having_feature(image_id)\n",
    "\n",
    "    _actual = list(map(lambda x: x.split(), captions_data.loc[captions_data['image'] == image_id]['caption'].tolist()))\n",
    "    _predicted = whole_text_output.split()\n",
    "    \n",
    "    actual.append(_actual)\n",
    "    predicted.append(_predicted)\n",
    "\n",
    "    logger.info(f\"\\n\\nActual: {_actual} \\n --- \\nPredicted: {_predicted}\")\n",
    "    \n",
    "score = corpus_bleu(actual, predicted)\n",
    "\n",
    "logger.info(f\"Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with strategy.scope() if USE_MULTIPLE_GPUS else nullcontext():\n",
    "    model = tf.keras.models.load_model('temp_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_parallelized(images, batch_size):\n",
    "\n",
    "    predicted: list[str] = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(images), batch_size)):\n",
    "        images_batch = images[i: i + batch_size]\n",
    "        num_images_in_batch = len(images_batch)\n",
    "        \n",
    "        text_inputs = [\"<start>\" for _ in range(num_images_in_batch)]\n",
    "        text_outputs = [\"\" for _ in range(num_images_in_batch)]\n",
    "\n",
    "        sequence_inputs = np.zeros(shape=(num_images_in_batch, absolute_max_length))\n",
    "        feature_inputs = np.zeros(shape=(num_images_in_batch, FEATURE_SHAPE[1]))\n",
    "\n",
    "        for j in range(absolute_max_length):\n",
    "            for k, image in enumerate(images_batch):\n",
    "                sequence_inputs[k] = default_vocab_handler.text_to_sequence(text_inputs[k], absolute_max_length, True)\n",
    "                feature_inputs[k] = features[image]\n",
    "\n",
    "            model_inputs = [feature_inputs, sequence_inputs]\n",
    "            \n",
    "            sequence_outputs = list(map(lambda x: np.argmax(x), model.predict(model_inputs, verbose=0)))\n",
    "            \n",
    "            text_output = list(map(lambda x: default_vocab_handler.word_of(x), sequence_outputs))\n",
    "\n",
    "            for l in range(num_images_in_batch):\n",
    "                text_outputs[l] += \" \" + text_output[l]\n",
    "                text_inputs[l] += \" \" + text_output[l]\n",
    "\n",
    "        predicted.extend(text_outputs)\n",
    "\n",
    "    return predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "with strategy.scope() if USE_MULTIPLE_GPUS else nullcontext():\n",
    "    predicted = test_parallelized(test, batch_size)\n",
    "\n",
    "actual = []\n",
    "for i, image_id in tqdm(enumerate(test)):\n",
    "    # since all the inference is run up to maxlength, the generated text\n",
    "    # might contain many <stop> i.e. stop words\n",
    "    # so discarding after encountering the first stop word\n",
    "    first_index_of_end_sequence = predicted[i].find(default_vocab_handler.stop_word)\n",
    "    predicted[i] = predicted[i][0: first_index_of_end_sequence].split()\n",
    "    \n",
    "    _actual = list(map(lambda x: x.split(), captions_data.loc[captions_data['image'] == image_id]['caption'].tolist()))\n",
    "    actual.append(_actual)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "score = corpus_bleu(actual, predicted)\n",
    "logger.info(f\"Bleu score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_generator.__next__()[]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 623329,
     "sourceId": 1111749,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6212676,
     "sourceId": 10078191,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
